{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "from load_quantized_model import load_model\n",
    "from datautils import *\n",
    "from eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='LlaMa model to load; pass location of hugginface converted checkpoint.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, \n",
    "    help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "# Quantization Method\n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing weight; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--abits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing activation; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--exponential', action='store_true',\n",
    "    help='Whether to use exponent-only for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static', action='store_true',\n",
    "    help='Whether to perform static quantization (For activtions). Default is dynamic. (Deprecated in Atom)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight_group_size', type=int, default=0, choices=[0, 32, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing weights. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument( #- ??\n",
    "    '--weight_channel_group', type=int, default=1,\n",
    "    help='Group size of channels that will quantize together. (only for weights now)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_group_size', type=int, default=0, choices=[0, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing activations. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--reorder', action='store_true',\n",
    "    help='Whether to keep salient weight unquantized.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_sort_metric', type=str, default='hessian', choices=['abs_mean', 'hessian'],\n",
    "    help='The metric used to sort the activations.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper', type=int, default=0,\n",
    "    help='Group size to keep outliers.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper_precision', type=int, default=0, choices=[0, 1, 2, 3],\n",
    "    help='Precision to keep outliers. 0 for FP16; 1 for E5M2; 2 for E4M3; 3 for INT8 Quant.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--cache_index', action='store_true',\n",
    "    help='Whether to use cached reorder index'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tiling', type=int, default=0, choices=[0, 16],\n",
    "    help='Tile-wise quantization granularity (Deprecated in Atom).'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_cache', action='store_true',\n",
    "    help='Whether to quant KV_Cache'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--use_gptq', action='store_true',\n",
    "    help='Whether to use GPTQ for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for activation quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for weight quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for kv cache quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_ppl\", action=\"store_true\",\n",
    "    help='Whether to evaluate perplexity.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_common_sense\", action=\"store_true\",\n",
    "    help='Whether to evaluate zero-shot accuray on commonsense reasoning tasks.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multigpu\", action=\"store_true\", \n",
    "    help=\"at eval, map model to multiple gpus\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_num_fewshot\", type=int, default=0, \n",
    "    help=\"Number of shots in lm evaluation. Default is 0 for zero-shot.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_limit\", type=int, default=-1, \n",
    "    help=\"Limit the number of examples in lm evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_dir', type=str, default='./saved',\n",
    "    help='Path to store the reordering indices and quantized weights.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--quant_type', type=str, default='int', choices=['int', 'fp'],\n",
    "    help='Determine the mapped data format by quant_type + n_bits. e.g. int8, fp4.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_model', action=\"store_true\", default=True,\n",
    "    help='Whether to save the quantized model.'\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\n",
    "  args = [\n",
    "    \"/root/project/Atom/llama2-7b\",\n",
    "    \"wikitext2\",\n",
    "    \"--wbits\", \"4\", \"--abits\", \"4\", \"--a_sym\", \"--w_sym\", \"--save_model\",\n",
    "    \"--act_group_size\", \"128\", \"--weight_group_size\", \"128\", \"--weight_channel_group\", \"2\",\n",
    "    \"--reorder\", \"--act_sort_metric\", \"hessian\", \"--cache_index\",\n",
    "    \"--a_clip_ratio\", \"0.9\", \"--w_clip_ratio\", \"0.85\", \"--kv_clip_ratio\", \"1.0\",\n",
    "    \"--keeper\", \"128\", \"--keeper_precision\", \"3\", \"--kv_cache\", \"--use_gptq\",\n",
    "    \"--eval_ppl\"\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ffc3c7fd7544cab61916fe63069d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached reording index from disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 2110.94it/s]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "DEV = torch.device('cuda:0')\n",
    "model = load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wikitext2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:07<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetResult,wikitext2,6.025\n"
     ]
    }
   ],
   "source": [
    "datasets = ['wikitext2']\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluating {dataset} ...\")\n",
    "    ppl = llama_eval(model, testloader, DEV)\n",
    "\n",
    "    print(f\"targetResult,{dataset},{ppl:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze\n",
    "from quant import *\n",
    "from outlier import *\n",
    "from eval import *\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from modelutils_llama import quantize_model_llama, reorder_model_llama, quantize_model_gptq_llama,  add_act_quant_wrapper_llama\n",
    "from modelutils_opt import quantize_model_opt, reorder_model_opt, quantize_model_gptq_opt,  add_act_quant_wrapper_opt\n",
    "from modelutils_mixtral import quantize_model_mixtral, add_act_quant_wrapper_mixtral, reorder_model_mixtral\n",
    "from parallel_utils import map_layers_to_multi_gpus\n",
    "from LMClass import LMClass\n",
    "from eval import pattern_match\n",
    "from lm_eval import tasks as lm_tasks\n",
    "from lm_eval import evaluator as lm_evaluator\n",
    "from datautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./saved/llama2-7b_quantized.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "inputs = torch.load(\"./saved/llama2-7b_inputs.pth\")\n",
    "outputs = torch.load(\"./saved/llama2-7b_outputs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scales = analyze.get_input_quant_param_dict(inputs)\n",
    "weight_scales = analyze.get_weight_quant_param_dict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test get scale functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = inputs[\"layers.1.self_attn.q_proj.input\"][0]\n",
    "test_weight = model.model.layers[1].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 4096])\n",
      "torch.Size([2048, 32])\n"
     ]
    }
   ],
   "source": [
    "test_input_scale = analyze.get_input_qunat_scale(test_input)\n",
    "print(test_input.shape)\n",
    "print(test_input_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n",
      "torch.Size([4096, 32])\n"
     ]
    }
   ],
   "source": [
    "test_weight_scale = analyze.get_weight_quant_scale(test_weight)\n",
    "print(test_weight.shape)\n",
    "print(test_weight_scale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q proj example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = inputs[\"layers.0.self_attn.q_proj.input\"][0][0]\n",
    "weight = model.model.layers[0].self_attn.q_proj.weight\n",
    "input_scale = input_scales[\"layers.0.self_attn.q_proj.input.scale\"][0]\n",
    "weight_scale = weight_scales[\"layers.0.self_attn.q_proj.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_in_scale = torch.zeros_like(input)\n",
    "for i in range(input_scale.shape[1]):\n",
    "  for j in range(128):\n",
    "    ex_in_scale[:, 128*i+j] = input_scale[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_weight_scale = torch.zeros_like(weight)\n",
    "for i in range(weight_scale.shape[1]):\n",
    "  for j in range(128):\n",
    "    ex_weight_scale[:, 128*i+j] = weight_scale[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "qin = torch.round(input / ex_in_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw = torch.round(weight / ex_weight_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.zeros(2048, 4096)\n",
    "for i in range(int(4096/128)):\n",
    "  input = qin[:, 128*i:128*(i+1)]\n",
    "  weight = qw[:, 128*i:128*(i+1)]\n",
    "  scale_mat = torch.matmul(input_scale[:, i].reshape(-1, 1).float(), weight_scale[:, i].reshape(1, -1).float())\n",
    "  out_psum = torch.matmul(input.float(), weight.T.float()) *scale_mat\n",
    "  output += out_psum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1107788086, -0.4235839844,  0.7661132812,  ...,\n",
       "         -0.1196899414,  0.6264648438,  0.1623535156],\n",
       "        [-0.0408325195, -0.3610839844,  1.0117187500,  ...,\n",
       "         -0.0342712402,  0.0709228516,  0.3554687500],\n",
       "        [ 0.2474365234,  0.0408630371,  0.3076171875,  ...,\n",
       "         -0.0485229492, -0.1354980469,  0.5034179688],\n",
       "        ...,\n",
       "        [ 0.0054931641, -0.4492187500,  1.0253906250,  ...,\n",
       "         -0.0422668457,  0.0504150391,  0.2705078125],\n",
       "        [ 0.2177734375, -1.2353515625,  2.0117187500,  ...,\n",
       "         -0.1453857422,  0.4226074219, -0.6743164062],\n",
       "        [ 0.1894531250, -0.3427734375,  0.9541015625,  ...,\n",
       "         -0.0264587402,  0.0542907715,  0.2880859375]], dtype=torch.float16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"layers.0.self_attn.q_proj.output\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1104393676, -0.4228451848,  0.7616682053,  ...,\n",
       "         -0.1186680347,  0.6208702326,  0.1648930609],\n",
       "        [-0.0407502465, -0.3595485091,  1.0115396976,  ...,\n",
       "         -0.0342495590,  0.0719347894,  0.3554733098],\n",
       "        [ 0.2474404275,  0.0442094766,  0.3077886701,  ...,\n",
       "         -0.0486076251, -0.1366732419,  0.5032029152],\n",
       "        ...,\n",
       "        [ 0.0055385567, -0.4463545382,  1.0250202417,  ...,\n",
       "         -0.0422924943,  0.0516813807,  0.2705228627],\n",
       "        [ 0.2165240347, -1.2322372198,  2.0030839443,  ...,\n",
       "         -0.1454128325,  0.4207982123, -0.6710996628],\n",
       "        [ 0.1896059364, -0.3401398659,  0.9539331794,  ...,\n",
       "         -0.0264508091,  0.0557389781,  0.2881198823]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

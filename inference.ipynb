{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze\n",
    "from quant import *\n",
    "from outlier import *\n",
    "from eval import *\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from modelutils_llama import quantize_model_llama, reorder_model_llama, quantize_model_gptq_llama,  add_act_quant_wrapper_llama\n",
    "from modelutils_opt import quantize_model_opt, reorder_model_opt, quantize_model_gptq_opt,  add_act_quant_wrapper_opt\n",
    "from modelutils_mixtral import quantize_model_mixtral, add_act_quant_wrapper_mixtral, reorder_model_mixtral\n",
    "from parallel_utils import map_layers_to_multi_gpus\n",
    "from LMClass import LMClass\n",
    "from eval import pattern_match\n",
    "from lm_eval import tasks as lm_tasks\n",
    "from lm_eval import evaluator as lm_evaluator\n",
    "from datautils import *\n",
    "import qLinearLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = torch.device('cuda:0')\n",
    "model = torch.load(\"./saved/llama2-7b_quantized.pth\").to(DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.load(\"./saved/llama2-7b_inputs.pth\")\n",
    "outputs = torch.load(\"./saved/llama2-7b_outputs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wikitext2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:04<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetResult,wikitext2,6.025\n"
     ]
    }
   ],
   "source": [
    "datasets = ['wikitext2']\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=\"./llama2-7b\", seqlen=model.seqlen\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluating {dataset} ...\")\n",
    "    ppl = llama_eval(model, testloader, DEV)\n",
    "\n",
    "    print(f\"targetResult,{dataset},{ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='LlaMa model to load; pass location of hugginface converted checkpoint.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, \n",
    "    help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "# Quantization Method\n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing weight; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--abits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing activation; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--exponential', action='store_true',\n",
    "    help='Whether to use exponent-only for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static', action='store_true',\n",
    "    help='Whether to perform static quantization (For activtions). Default is dynamic. (Deprecated in Atom)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight_group_size', type=int, default=0, choices=[0, 32, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing weights. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument( #- ??\n",
    "    '--weight_channel_group', type=int, default=1,\n",
    "    help='Group size of channels that will quantize together. (only for weights now)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_group_size', type=int, default=0, choices=[0, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing activations. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--reorder', action='store_true',\n",
    "    help='Whether to keep salient weight unquantized.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_sort_metric', type=str, default='hessian', choices=['abs_mean', 'hessian'],\n",
    "    help='The metric used to sort the activations.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper', type=int, default=0,\n",
    "    help='Group size to keep outliers.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper_precision', type=int, default=0, choices=[0, 1, 2, 3],\n",
    "    help='Precision to keep outliers. 0 for FP16; 1 for E5M2; 2 for E4M3; 3 for INT8 Quant.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--cache_index', action='store_true',\n",
    "    help='Whether to use cached reorder index'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tiling', type=int, default=0, choices=[0, 16],\n",
    "    help='Tile-wise quantization granularity (Deprecated in Atom).'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_cache', action='store_true',\n",
    "    help='Whether to quant KV_Cache'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--use_gptq', action='store_true',\n",
    "    help='Whether to use GPTQ for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for activation quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for weight quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for kv cache quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_ppl\", action=\"store_true\",\n",
    "    help='Whether to evaluate perplexity.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_common_sense\", action=\"store_true\",\n",
    "    help='Whether to evaluate zero-shot accuray on commonsense reasoning tasks.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multigpu\", action=\"store_true\", \n",
    "    help=\"at eval, map model to multiple gpus\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_num_fewshot\", type=int, default=0, \n",
    "    help=\"Number of shots in lm evaluation. Default is 0 for zero-shot.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_limit\", type=int, default=-1, \n",
    "    help=\"Limit the number of examples in lm evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_dir', type=str, default='./saved',\n",
    "    help='Path to store the reordering indices and quantized weights.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--quant_type', type=str, default='int', choices=['int', 'fp'],\n",
    "    help='Determine the mapped data format by quant_type + n_bits. e.g. int8, fp4.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_model', action=\"store_true\", default=True,\n",
    "    help='Whether to save the quantized model.'\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\n",
    "  args = [\n",
    "    \"/root/project/Atom/llama2-7b\",\n",
    "    \"wikitext2\",\n",
    "    \"--wbits\", \"4\", \"--abits\", \"4\", \"--a_sym\", \"--w_sym\", \"--save_model\",\n",
    "    \"--act_group_size\", \"128\", \"--weight_group_size\", \"128\", \"--weight_channel_group\", \"2\",\n",
    "    \"--reorder\", \"--act_sort_metric\", \"hessian\", \"--cache_index\",\n",
    "    \"--a_clip_ratio\", \"0.9\", \"--w_clip_ratio\", \"0.85\", \"--kv_clip_ratio\", \"1.0\",\n",
    "    \"--keeper\", \"128\", \"--keeper_precision\", \"3\", \"--kv_cache\", \"--use_gptq\",\n",
    "    \"--eval_common_sense\", \"--lm_eval_limit\", \"-1\", \"--multigpu\"\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  32000\n",
      "map layer 0 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 1 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 2 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 3 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 4 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 5 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 6 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 7 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 8 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 9 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 10 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 11 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 12 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 13 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 14 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 15 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 16 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 17 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 18 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 19 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 20 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 21 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 22 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 23 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 24 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 25 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 26 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 27 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 28 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 29 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map layer 30 to gpu 0, [(0, 24576, 13504), (1, 24576, 14086), (2, 24576, 13578), (3, 24576, 13504)]\n",
      "map last layer 31 to gpu 0\n",
      "Selected Tasks: ['winogrande', 'hellaswag', 'boolq', 'arc_challenge', 'piqa', 'arc_easy']\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67078/67078 [2:14:11<00:00,  8.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': {'arc_challenge': {'acc': 0.3873720136518771,\n",
      "                               'acc_norm': 0.3822525597269625,\n",
      "                               'acc_norm_stderr': 0.014200454049979277,\n",
      "                               'acc_stderr': 0.014235872487909872},\n",
      "             'arc_easy': {'acc': 0.6574074074074074,\n",
      "                          'acc_norm': 0.5109427609427609,\n",
      "                          'acc_norm_stderr': 0.010257326131172868,\n",
      "                          'acc_stderr': 0.009738105469984193},\n",
      "             'boolq': {'acc': 0.6892966360856269,\n",
      "                       'acc_stderr': 0.008094100581882606},\n",
      "             'hellaswag': {'acc': 0.5343557060346544,\n",
      "                           'acc_norm': 0.6984664409480184,\n",
      "                           'acc_norm_stderr': 0.004579859084500781,\n",
      "                           'acc_stderr': 0.004977988452502639},\n",
      "             'piqa': {'acc': 0.7606093579978237,\n",
      "                      'acc_norm': 0.7470076169749728,\n",
      "                      'acc_norm_stderr': 0.010142888698862448,\n",
      "                      'acc_stderr': 0.00995588425029169},\n",
      "             'winogrande': {'acc': 0.6314127861089187,\n",
      "                            'acc_stderr': 0.013558447570099316}},\n",
      " 'versions': {'arc_challenge': 0,\n",
      "              'arc_easy': 0,\n",
      "              'boolq': 1,\n",
      "              'hellaswag': 0,\n",
      "              'piqa': 0,\n",
      "              'winogrande': 0}}\n",
      "INFO piqa : 74.70\n",
      "INFO arc_easy : 51.09\n",
      "INFO arc_challenge : 38.23\n",
      "INFO boolq : 68.93\n",
      "INFO hellaswag : 69.85\n",
      "INFO winogrande : 63.14\n"
     ]
    }
   ],
   "source": [
    "lm = LMClass(args, model)\n",
    "lm.seqlen = 2048\n",
    "lm.model.eval()\n",
    "for param in lm.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if args.multigpu:\n",
    "    if (\"llama\" in args.model.lower()) or (\"mixtral\" in args.model.lower()):\n",
    "        map_layers_to_multi_gpus(lm.model.model.layers)\n",
    "        input_device = lm.model.model.layers[0].device\n",
    "        output_device = lm.model.model.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.embed_tokens.to(input_device)\n",
    "        lm.model.model.norm.to(output_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "    elif \"opt\" in args.model.lower():\n",
    "        map_layers_to_multi_gpus(lm.model.model.decoder.layers)\n",
    "        input_device = lm.model.model.decoder.layers[0].device\n",
    "        output_device = lm.model.model.decoder.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.decoder.embed_tokens.to(input_device)\n",
    "        lm.model.model.decoder.embed_positions.to(input_device)\n",
    "        lm.model.model.decoder.final_layer_norm.to(input_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "else:\n",
    "    lm._device = DEV\n",
    "    lm.model = lm.model.to(lm.device)\n",
    "\n",
    "results = {}\n",
    "tasks_str = \"piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande\"\n",
    "task_names = pattern_match(tasks_str.split(\",\"), lm_tasks.ALL_TASKS)\n",
    "print(f\"Selected Tasks: {task_names}\")\n",
    "\n",
    "task_dict = lm_tasks.get_task_dict(task_names)\n",
    "t_results = lm_evaluator.evaluate(\n",
    "    lm,\n",
    "    task_dict,\n",
    "    num_fewshot=args.lm_eval_num_fewshot,\n",
    "    limit=None if args.lm_eval_limit == -1 else args.lm_eval_limit\n",
    ")\n",
    "results.update(t_results)\n",
    "pprint(results)\n",
    "\n",
    "results_dict = results['results']\n",
    "for task_name in tasks_str.split(','):\n",
    "    if task_name in ['piqa', 'arc_easy', 'arc_challenge', 'hellaswag']:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc_norm']*100:.2f}\")\n",
    "    else:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacement experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5220630169,  1.2500656843,  1.9160656929,  ...,\n",
      "          -0.3587054014, -0.6806795001, -0.5678879023],\n",
      "         [-0.5549470782,  0.3020634353,  0.2215119302,  ...,\n",
      "          -0.2485001683, -0.8814989328,  0.7776247263],\n",
      "         [-0.5584750175,  1.2743335962,  1.9794452190,  ...,\n",
      "          -0.3425338268, -0.7754001021, -0.6752771139],\n",
      "         ...,\n",
      "         [ 0.0430632308,  0.0613280237,  0.3762882352,  ...,\n",
      "           0.0973920226,  1.6304383278, -2.3274102211],\n",
      "         [ 0.1496249735, -0.3039065003,  0.1800004691,  ...,\n",
      "           0.7147076726, -0.3562183976, -1.8467016220],\n",
      "         [-0.5398339033,  0.1594856083,  0.2489071935,  ...,\n",
      "          -0.6237655282,  0.5928454399, -2.6226317883]]])\n",
      "tensor([[[-0.5219924450,  1.2499254942,  1.9160754681,  ...,\n",
      "          -0.3581647277, -0.6810475588, -0.5687916279],\n",
      "         [-0.5553250313,  0.3030800223,  0.2204785347,  ...,\n",
      "          -0.2485207915, -0.8809019923,  0.7772969007],\n",
      "         [-0.5583836436,  1.2744653225,  1.9781659842,  ...,\n",
      "          -0.3427138925, -0.7756060958, -0.6754946709],\n",
      "         ...,\n",
      "         [ 0.0429766774,  0.0625847578,  0.3746577501,  ...,\n",
      "           0.0987401009,  1.6304903030, -2.3276438713],\n",
      "         [ 0.1490575075, -0.3044664264,  0.1782464981,  ...,\n",
      "           0.7153844237, -0.3581116199, -1.8467948437],\n",
      "         [-0.5406918526,  0.1621351242,  0.2477763295,  ...,\n",
      "          -0.6243242025,  0.5949373245, -2.6227807999]]], device='cuda:0')\n",
      "tensor(453)\n"
     ]
    }
   ],
   "source": [
    "layer_num = 28\n",
    "\n",
    "test_input = inputs[f\"layers.{layer_num}.self_attn.q_proj.input\"][0].float()\n",
    "test_weight = model.model.layers[layer_num].self_attn.q_proj.weight.cpu().float()\n",
    "test_ref_output = torch.functional.F.linear(test_input, test_weight, None)\n",
    "\n",
    "layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "layer_v2.args = model.model.layers[layer_num].self_attn.q_proj.args\n",
    "layer_v2.weight = model.model.layers[layer_num].self_attn.q_proj.weight\n",
    "layer_v2.bias = model.model.layers[layer_num].self_attn.q_proj.bias\n",
    "layer_v2 = layer_v2.to(DEV)\n",
    "test_eval_output = layer_v2(test_input.to(DEV))\n",
    "\n",
    "print(test_ref_output)\n",
    "print(test_eval_output)\n",
    "\n",
    "print(((test_ref_output - test_eval_output.cpu()) > 4e-2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_layers = {}\n",
    "for name, m in model.model.named_modules():\n",
    "    if isinstance(m, qLinearLayer.QLinearLayer):\n",
    "      layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "      layer_v2.args = m.args\n",
    "      layer_v2.weight = m.weight\n",
    "      layer_v2.bias = m.bias\n",
    "      changed_layers[name] = layer_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in changed_layers.items():\n",
    "    analyze.set_nested_attr(model.model, name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x QLlamaDecoderLayer(\n",
      "        (self_attn): QLlamaAttention(\n",
      "          (q_proj): QLinearLayerV2()\n",
      "          (k_proj): QLinearLayerV2()\n",
      "          (v_proj): QLinearLayerV2()\n",
      "          (o_proj): QLinearLayerV2()\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (act_quant): Quantizer()\n",
      "          (v_quant): Quantizer()\n",
      "          (k_quant): Quantizer()\n",
      "        )\n",
      "        (mlp): QLlamaMLP(\n",
      "          (gate_proj): QLinearLayerV2()\n",
      "          (down_proj): QLinearLayerV2()\n",
      "          (up_proj): QLinearLayerV2()\n",
      "          (act_fn): SiLU()\n",
      "          (act_quant): Quantizer()\n",
      "        )\n",
      "        (input_layernorm): QLlamaRMSNorm(\n",
      "          (originalNorm): LlamaRMSNorm()\n",
      "          (act_quant): Quantizer()\n",
      "        )\n",
      "        (post_attention_layernorm): QLlamaRMSNorm(\n",
      "          (originalNorm): LlamaRMSNorm()\n",
      "          (act_quant): Quantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wikitext2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [1:20:27<00:00, 150.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetResult,wikitext2,nan\n"
     ]
    }
   ],
   "source": [
    "datasets = ['wikitext2']\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=\"./llama2-7b\", seqlen=model.seqlen\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluating {dataset} ...\")\n",
    "    ppl = llama_eval(model, testloader, DEV)\n",
    "\n",
    "    print(f\"targetResult,{dataset},{ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  32000\n",
      "map layer 0 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 1 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 2 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 3 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 4 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 5 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 6 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 7 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 8 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 9 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 10 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 11 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 12 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 13 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 14 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 15 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 16 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 17 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 18 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 19 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 20 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 21 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 22 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 23 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 24 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 25 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 26 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 27 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 28 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 29 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 30 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map last layer 31 to gpu 2\n",
      "Selected Tasks: ['winogrande']\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1018/2534 [5:43:46<8:31:56, 20.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 190\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected Tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m task_dict \u001b[38;5;241m=\u001b[39m lm_tasks\u001b[38;5;241m.\u001b[39mget_task_dict(task_names)\n\u001b[0;32m--> 190\u001b[0m t_results \u001b[38;5;241m=\u001b[39m \u001b[43mlm_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fewshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_eval_num_fewshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_eval_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_eval_limit\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m results\u001b[38;5;241m.\u001b[39mupdate(t_results)\n\u001b[1;32m    197\u001b[0m pprint(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/utils.py:161\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/evaluator.py:247\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(lm, task_dict, provide_description, num_fewshot, limit, bootstrap_iters, description_dict, decontamination_ngrams_path)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reqtype, reqs \u001b[38;5;129;01min\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# TODO: right now, this code runs multiple separate LM requests for multiple Requests differing\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#       only in index. We could implement some kind of caching, but that would be more of a band-aid\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m#       solution. we could also implement some kind of auto-grouping here;\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m#       they should end up next to each other.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning\u001b[39m\u001b[38;5;124m\"\u001b[39m, reqtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m     resps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreqs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     resps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    249\u001b[0m         x \u001b[38;5;28;01mif\u001b[39;00m req\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x[req\u001b[38;5;241m.\u001b[39mindex] \u001b[38;5;28;01mfor\u001b[39;00m x, req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, reqs)\n\u001b[1;32m    250\u001b[0m     ]\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resp, (i, task_name, doc, doc_id) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, requests_origin[reqtype]):\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/base.py:185\u001b[0m, in \u001b[0;36mBaseLM.loglikelihood\u001b[0;34m(self, requests)\u001b[0m\n\u001b[1;32m    181\u001b[0m     continuation_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_encode(continuation)\n\u001b[1;32m    183\u001b[0m     new_reqs\u001b[38;5;241m.\u001b[39mappend(((context, continuation), context_enc, continuation_enc))\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loglikelihood_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/base.py:295\u001b[0m, in \u001b[0;36mBaseLM._loglikelihood_tokens\u001b[0;34m(self, requests, disable_tqdm)\u001b[0m\n\u001b[1;32m    291\u001b[0m     inplens\u001b[38;5;241m.\u001b[39mappend(inplen)\n\u001b[1;32m    293\u001b[0m batched_inps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inps, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [batch, padding_length\u001b[39;00m\n\u001b[1;32m    294\u001b[0m multi_logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inps\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    296\u001b[0m )\u001b[38;5;241m.\u001b[39mcpu()  \u001b[38;5;66;03m# [batch, padding_length, vocab]\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (cache_key, _, _), logits, inp, inplen, cont_toks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    299\u001b[0m     chunk, multi_logits, inps, inplens, cont_toks_list\n\u001b[1;32m    300\u001b[0m ):\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# Slice to original seq length\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     contlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cont_toks)\n",
      "File \u001b[0;32m~/project/Atom/./model/LMClass.py:108\u001b[0m, in \u001b[0;36mLMClass._model_call\u001b[0;34m(self, inps)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03minps: a torch tensor of shape [batch, sequence]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mthe size of sequence may vary from call to call\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mreturns: a torch tensor of shape [batch, sequence, vocab] with the\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mlogits returned from the model\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1196\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         cache_position,\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/Atom/./model/qLlamaLayer.py:116\u001b[0m, in \u001b[0;36mQLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, cache_position)\u001b[0m\n\u001b[1;32m    114\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    115\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 116\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    119\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/Atom/./model/qLlamaLayer.py:351\u001b[0m, in \u001b[0;36mQLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# Quantize the activations and feed into down_proj\u001b[39;00m\n\u001b[1;32m    350\u001b[0m tmpResult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_quant(tmpResult)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmpResult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/Atom/./model/qLinearLayer.py:103\u001b[0m, in \u001b[0;36mQLinearLayerV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     out_shape \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 103\u001b[0m     weight_scales \u001b[38;5;241m=\u001b[39m analyze\u001b[38;5;241m.\u001b[39mget_weight_quant_scale(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    104\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(out_shape, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='LlaMa model to load; pass location of hugginface converted checkpoint.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, \n",
    "    help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "# Quantization Method\n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing weight; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--abits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing activation; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--exponential', action='store_true',\n",
    "    help='Whether to use exponent-only for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static', action='store_true',\n",
    "    help='Whether to perform static quantization (For activtions). Default is dynamic. (Deprecated in Atom)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight_group_size', type=int, default=0, choices=[0, 32, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing weights. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument( #- ??\n",
    "    '--weight_channel_group', type=int, default=1,\n",
    "    help='Group size of channels that will quantize together. (only for weights now)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_group_size', type=int, default=0, choices=[0, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing activations. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--reorder', action='store_true',\n",
    "    help='Whether to keep salient weight unquantized.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_sort_metric', type=str, default='hessian', choices=['abs_mean', 'hessian'],\n",
    "    help='The metric used to sort the activations.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper', type=int, default=0,\n",
    "    help='Group size to keep outliers.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper_precision', type=int, default=0, choices=[0, 1, 2, 3],\n",
    "    help='Precision to keep outliers. 0 for FP16; 1 for E5M2; 2 for E4M3; 3 for INT8 Quant.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--cache_index', action='store_true',\n",
    "    help='Whether to use cached reorder index'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tiling', type=int, default=0, choices=[0, 16],\n",
    "    help='Tile-wise quantization granularity (Deprecated in Atom).'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_cache', action='store_true',\n",
    "    help='Whether to quant KV_Cache'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--use_gptq', action='store_true',\n",
    "    help='Whether to use GPTQ for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for activation quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for weight quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for kv cache quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_ppl\", action=\"store_true\",\n",
    "    help='Whether to evaluate perplexity.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_common_sense\", action=\"store_true\",\n",
    "    help='Whether to evaluate zero-shot accuray on commonsense reasoning tasks.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multigpu\", action=\"store_true\", \n",
    "    help=\"at eval, map model to multiple gpus\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_num_fewshot\", type=int, default=0, \n",
    "    help=\"Number of shots in lm evaluation. Default is 0 for zero-shot.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_limit\", type=int, default=-1, \n",
    "    help=\"Limit the number of examples in lm evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_dir', type=str, default='./saved',\n",
    "    help='Path to store the reordering indices and quantized weights.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--quant_type', type=str, default='int', choices=['int', 'fp'],\n",
    "    help='Determine the mapped data format by quant_type + n_bits. e.g. int8, fp4.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_model', action=\"store_true\", default=True,\n",
    "    help='Whether to save the quantized model.'\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\n",
    "  args = [\n",
    "    \"/root/project/Atom/llama2-7b\",\n",
    "    \"wikitext2\",\n",
    "    \"--wbits\", \"4\", \"--abits\", \"4\", \"--a_sym\", \"--w_sym\", \"--save_model\",\n",
    "    \"--act_group_size\", \"128\", \"--weight_group_size\", \"128\", \"--weight_channel_group\", \"2\",\n",
    "    \"--reorder\", \"--act_sort_metric\", \"hessian\", \"--cache_index\",\n",
    "    \"--a_clip_ratio\", \"0.9\", \"--w_clip_ratio\", \"0.85\", \"--kv_clip_ratio\", \"1.0\",\n",
    "    \"--keeper\", \"128\", \"--keeper_precision\", \"3\", \"--kv_cache\", \"--use_gptq\",\n",
    "    \"--eval_common_sense\", \"--lm_eval_limit\", \"-1\", \"--multigpu\"\n",
    "  ]\n",
    ")\n",
    "\n",
    "lm = LMClass(args, model)\n",
    "lm.seqlen = 2048\n",
    "lm.model.eval()\n",
    "for param in lm.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if args.multigpu:\n",
    "    if (\"llama\" in args.model.lower()) or (\"mixtral\" in args.model.lower()):\n",
    "        map_layers_to_multi_gpus(lm.model.model.layers)\n",
    "        input_device = lm.model.model.layers[0].device\n",
    "        output_device = lm.model.model.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.embed_tokens.to(input_device)\n",
    "        lm.model.model.norm.to(output_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "    elif \"opt\" in args.model.lower():\n",
    "        map_layers_to_multi_gpus(lm.model.model.decoder.layers)\n",
    "        input_device = lm.model.model.decoder.layers[0].device\n",
    "        output_device = lm.model.model.decoder.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.decoder.embed_tokens.to(input_device)\n",
    "        lm.model.model.decoder.embed_positions.to(input_device)\n",
    "        lm.model.model.decoder.final_layer_norm.to(input_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "else:\n",
    "    lm._device = DEV\n",
    "    lm.model = lm.model.to(lm.device)\n",
    "\n",
    "results = {}\n",
    "# tasks_str = \"piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande\"\n",
    "# tasks_str = \"hellaswag,winogrande\"\n",
    "# tasks_str = \"hellaswag\"\n",
    "tasks_str = \"winogrande\"\n",
    "task_names = pattern_match(tasks_str.split(\",\"), lm_tasks.ALL_TASKS)\n",
    "print(f\"Selected Tasks: {task_names}\")\n",
    "\n",
    "task_dict = lm_tasks.get_task_dict(task_names)\n",
    "t_results = lm_evaluator.evaluate(\n",
    "    lm,\n",
    "    task_dict,\n",
    "    num_fewshot=args.lm_eval_num_fewshot,\n",
    "    limit=None if args.lm_eval_limit == -1 else args.lm_eval_limit\n",
    ")\n",
    "results.update(t_results)\n",
    "pprint(results)\n",
    "\n",
    "results_dict = results['results']\n",
    "for task_name in tasks_str.split(','):\n",
    "    if task_name in ['piqa', 'arc_easy', 'arc_challenge', 'hellaswag']:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc_norm']*100:.2f}\")\n",
    "    else:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ACIM GEMM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

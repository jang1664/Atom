{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze\n",
    "from quant import *\n",
    "from outlier import *\n",
    "from eval import *\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from modelutils_llama import quantize_model_llama, reorder_model_llama, quantize_model_gptq_llama,  add_act_quant_wrapper_llama\n",
    "from modelutils_opt import quantize_model_opt, reorder_model_opt, quantize_model_gptq_opt,  add_act_quant_wrapper_opt\n",
    "from modelutils_mixtral import quantize_model_mixtral, add_act_quant_wrapper_mixtral, reorder_model_mixtral\n",
    "from parallel_utils import map_layers_to_multi_gpus\n",
    "from LMClass import LMClass\n",
    "from eval import pattern_match\n",
    "from lm_eval import tasks as lm_tasks\n",
    "from lm_eval import evaluator as lm_evaluator\n",
    "from datautils import *\n",
    "import qLinearLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = torch.device('cuda:3')\n",
    "model = torch.load(\"./saved/llama2-7b_quantized.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./saved/llama2-7b_inputs.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./saved/llama2-7b_outputs.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/serialization.py:1357\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# inputs = torch.load(\"./saved/llama2-7b_inputs.pth\")\n",
    "# outputs = torch.load(\"./saved/llama2-7b_outputs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wikitext2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:16<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetResult,wikitext2,6.027\n"
     ]
    }
   ],
   "source": [
    "datasets = ['wikitext2']\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=\"./llama2-7b\", seqlen=model.seqlen\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluating {dataset} ...\")\n",
    "    ppl = llama_eval(model, testloader, DEV)\n",
    "\n",
    "    print(f\"targetResult,{dataset},{ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='LlaMa model to load; pass location of hugginface converted checkpoint.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, \n",
    "    help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "# Quantization Method\n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing weight; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--abits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing activation; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--exponential', action='store_true',\n",
    "    help='Whether to use exponent-only for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static', action='store_true',\n",
    "    help='Whether to perform static quantization (For activtions). Default is dynamic. (Deprecated in Atom)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight_group_size', type=int, default=0, choices=[0, 32, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing weights. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument( #- ??\n",
    "    '--weight_channel_group', type=int, default=1,\n",
    "    help='Group size of channels that will quantize together. (only for weights now)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_group_size', type=int, default=0, choices=[0, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing activations. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--reorder', action='store_true',\n",
    "    help='Whether to keep salient weight unquantized.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_sort_metric', type=str, default='hessian', choices=['abs_mean', 'hessian'],\n",
    "    help='The metric used to sort the activations.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper', type=int, default=0,\n",
    "    help='Group size to keep outliers.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper_precision', type=int, default=0, choices=[0, 1, 2, 3],\n",
    "    help='Precision to keep outliers. 0 for FP16; 1 for E5M2; 2 for E4M3; 3 for INT8 Quant.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--cache_index', action='store_true',\n",
    "    help='Whether to use cached reorder index'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tiling', type=int, default=0, choices=[0, 16],\n",
    "    help='Tile-wise quantization granularity (Deprecated in Atom).'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_cache', action='store_true',\n",
    "    help='Whether to quant KV_Cache'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--use_gptq', action='store_true',\n",
    "    help='Whether to use GPTQ for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for activation quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for weight quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for kv cache quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_ppl\", action=\"store_true\",\n",
    "    help='Whether to evaluate perplexity.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_common_sense\", action=\"store_true\",\n",
    "    help='Whether to evaluate zero-shot accuray on commonsense reasoning tasks.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multigpu\", action=\"store_true\", \n",
    "    help=\"at eval, map model to multiple gpus\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_num_fewshot\", type=int, default=0, \n",
    "    help=\"Number of shots in lm evaluation. Default is 0 for zero-shot.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_limit\", type=int, default=-1, \n",
    "    help=\"Limit the number of examples in lm evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_dir', type=str, default='./saved',\n",
    "    help='Path to store the reordering indices and quantized weights.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--quant_type', type=str, default='int', choices=['int', 'fp'],\n",
    "    help='Determine the mapped data format by quant_type + n_bits. e.g. int8, fp4.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_model', action=\"store_true\", default=True,\n",
    "    help='Whether to save the quantized model.'\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\n",
    "  args = [\n",
    "    \"/root/project/Atom/llama2-7b\",\n",
    "    \"wikitext2\",\n",
    "    \"--wbits\", \"4\", \"--abits\", \"4\", \"--a_sym\", \"--w_sym\", \"--save_model\",\n",
    "    \"--act_group_size\", \"128\", \"--weight_group_size\", \"128\", \"--weight_channel_group\", \"2\",\n",
    "    \"--reorder\", \"--act_sort_metric\", \"hessian\", \"--cache_index\",\n",
    "    \"--a_clip_ratio\", \"0.9\", \"--w_clip_ratio\", \"0.85\", \"--kv_clip_ratio\", \"1.0\",\n",
    "    \"--keeper\", \"128\", \"--keeper_precision\", \"3\", \"--kv_cache\", \"--use_gptq\",\n",
    "    \"--eval_common_sense\", \"--lm_eval_limit\", \"-1\", \"--multigpu\"\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  32000\n",
      "map layer 0 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 322), (3, 24576, 8123)]\n",
      "map layer 1 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 991), (3, 24576, 8123)]\n",
      "map layer 2 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 1381), (3, 24576, 8123)]\n",
      "map layer 3 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 1771), (3, 24576, 8123)]\n",
      "map layer 4 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 2161), (3, 24576, 8123)]\n",
      "map layer 5 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 2571), (3, 24576, 8123)]\n",
      "map layer 6 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 2961), (3, 24576, 8123)]\n",
      "map layer 7 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 3351), (3, 24576, 8123)]\n",
      "map layer 8 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 3743), (3, 24576, 8123)]\n",
      "map layer 9 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 4133), (3, 24576, 8123)]\n",
      "map layer 10 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 4543), (3, 24576, 8123)]\n",
      "map layer 11 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 4933), (3, 24576, 8123)]\n",
      "map layer 12 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 5323), (3, 24576, 8123)]\n",
      "map layer 13 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 5713), (3, 24576, 8123)]\n",
      "map layer 14 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 6103), (3, 24576, 8123)]\n",
      "map layer 15 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 6515), (3, 24576, 8123)]\n",
      "map layer 16 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 6905), (3, 24576, 8123)]\n",
      "map layer 17 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 7295), (3, 24576, 8123)]\n",
      "map layer 18 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 7685), (3, 24576, 8123)]\n",
      "map layer 19 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 8095), (3, 24576, 8123)]\n",
      "map layer 20 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 8485), (3, 24576, 8123)]\n",
      "map layer 21 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 8875), (3, 24576, 8123)]\n",
      "map layer 22 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 9267), (3, 24576, 8123)]\n",
      "map layer 23 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 9657), (3, 24576, 8123)]\n",
      "map layer 24 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 10067), (3, 24576, 8123)]\n",
      "map layer 25 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 10457), (3, 24576, 8123)]\n",
      "map layer 26 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 10847), (3, 24576, 8123)]\n",
      "map layer 27 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 11237), (3, 24576, 8123)]\n",
      "map layer 28 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 11627), (3, 24576, 8123)]\n",
      "map layer 29 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 12037), (3, 24576, 8123)]\n",
      "map layer 30 to gpu 2, [(0, 24576, 897), (1, 24576, 330), (2, 24576, 12429), (3, 24576, 8123)]\n",
      "map last layer 31 to gpu 2\n",
      "Selected Tasks: ['winogrande', 'hellaswag', 'piqa', 'boolq', 'arc_challenge', 'arc_easy']\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67078/67078 [1:57:03<00:00,  9.55it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': {'arc_challenge': {'acc': 0.39334470989761094,\n",
      "                               'acc_norm': 0.3924914675767918,\n",
      "                               'acc_norm_stderr': 0.01426963463567072,\n",
      "                               'acc_stderr': 0.014275101465693026},\n",
      "             'arc_easy': {'acc': 0.6691919191919192,\n",
      "                          'acc_norm': 0.5214646464646465,\n",
      "                          'acc_norm_stderr': 0.010250325159456649,\n",
      "                          'acc_stderr': 0.00965454012598612},\n",
      "             'boolq': {'acc': 0.67217125382263,\n",
      "                       'acc_stderr': 0.00821024323767339},\n",
      "             'hellaswag': {'acc': 0.5371439952200757,\n",
      "                           'acc_norm': 0.6964748058155746,\n",
      "                           'acc_norm_stderr': 0.004588403419449682,\n",
      "                           'acc_stderr': 0.004975993795562027},\n",
      "             'piqa': {'acc': 0.7573449401523396,\n",
      "                      'acc_norm': 0.7546245919477693,\n",
      "                      'acc_norm_stderr': 0.010039831320422393,\n",
      "                      'acc_stderr': 0.0100020025697087},\n",
      "             'winogrande': {'acc': 0.6393054459352802,\n",
      "                            'acc_stderr': 0.013496064394234026}},\n",
      " 'versions': {'arc_challenge': 0,\n",
      "              'arc_easy': 0,\n",
      "              'boolq': 1,\n",
      "              'hellaswag': 0,\n",
      "              'piqa': 0,\n",
      "              'winogrande': 0}}\n",
      "INFO piqa : 75.46\n",
      "INFO arc_easy : 52.15\n",
      "INFO arc_challenge : 39.25\n",
      "INFO boolq : 67.22\n",
      "INFO hellaswag : 69.65\n",
      "INFO winogrande : 63.93\n"
     ]
    }
   ],
   "source": [
    "lm = LMClass(args, model)\n",
    "lm.seqlen = 2048\n",
    "lm.model.eval()\n",
    "for param in lm.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if args.multigpu:\n",
    "    if (\"llama\" in args.model.lower()) or (\"mixtral\" in args.model.lower()):\n",
    "        map_layers_to_multi_gpus(lm.model.model.layers)\n",
    "        input_device = lm.model.model.layers[0].device\n",
    "        output_device = lm.model.model.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.embed_tokens.to(input_device)\n",
    "        lm.model.model.norm.to(output_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "    elif \"opt\" in args.model.lower():\n",
    "        map_layers_to_multi_gpus(lm.model.model.decoder.layers)\n",
    "        input_device = lm.model.model.decoder.layers[0].device\n",
    "        output_device = lm.model.model.decoder.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.decoder.embed_tokens.to(input_device)\n",
    "        lm.model.model.decoder.embed_positions.to(input_device)\n",
    "        lm.model.model.decoder.final_layer_norm.to(input_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "else:\n",
    "    lm._device = DEV\n",
    "    lm.model = lm.model.to(lm.device)\n",
    "\n",
    "results = {}\n",
    "tasks_str = \"piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande\"\n",
    "task_names = pattern_match(tasks_str.split(\",\"), lm_tasks.ALL_TASKS)\n",
    "print(f\"Selected Tasks: {task_names}\")\n",
    "\n",
    "task_dict = lm_tasks.get_task_dict(task_names)\n",
    "t_results = lm_evaluator.evaluate(\n",
    "    lm,\n",
    "    task_dict,\n",
    "    num_fewshot=args.lm_eval_num_fewshot,\n",
    "    limit=None if args.lm_eval_limit == -1 else args.lm_eval_limit\n",
    ")\n",
    "results.update(t_results)\n",
    "pprint(results)\n",
    "\n",
    "results_dict = results['results']\n",
    "for task_name in tasks_str.split(','):\n",
    "    if task_name in ['piqa', 'arc_easy', 'arc_challenge', 'hellaswag']:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc_norm']*100:.2f}\")\n",
    "    else:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacement experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max : 0.39823007583618164, mean : 0.0014240944292396307, std : 0.014736422337591648\n"
     ]
    }
   ],
   "source": [
    "errs = torch.zeros(1000)\n",
    "for i in range(1000):\n",
    "  a = torch.randint(-8, 8, (4096,)).to(torch.float16)\n",
    "  b = torch.randint(-8, 8, (4096,)).to(torch.float16)\n",
    "  sa = torch.rand(1).to(torch.float16)\n",
    "  sb = torch.rand(1).to(torch.float16)\n",
    "  out_1 = ((a*sa) * (b*sb)).sum()\n",
    "  out_2 = (a * b).sum() * (sa * sb)\n",
    "\n",
    "  err = torch.abs(out_1.float() - out_2.float())/torch.abs(out_1.float())\n",
    "  errs[i] = err\n",
    "print(f\"max : {errs.max()}, mean : {errs.mean()}, std : {errs.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH INT Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy:\n",
    "  def __init__(self):\n",
    "    self.weight = None\n",
    "    self.weight_scale = None\n",
    "    self.bias = None\n",
    "    self.args = None\n",
    "    self.enable_quant = None\n",
    "    self.weight_int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 0 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 3.000000e+00 : 3.000000e+00\n",
      "Try 1 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 3.590000e+02 : 3.590000e+02\n",
      "Try 2 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -2.790000e+02 : -2.790000e+02\n",
      "Try 3 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 7.000000e+01 : 7.000000e+01\n",
      "Try 4 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 4.220000e+02 : 4.220000e+02\n",
      "Try 5 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.470000e+02 : -1.470000e+02\n",
      "Try 6 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -3.940000e+02 : -3.940000e+02\n",
      "Try 7 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -2.730000e+02 : -2.730000e+02\n",
      "Try 8 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 3.500000e+01 : 3.500000e+01\n",
      "Try 9 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -2.220000e+02 : -2.220000e+02\n",
      "Try 10 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.160000e+02 : -1.160000e+02\n",
      "Try 11 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 2.490000e+02 : 2.490000e+02\n",
      "Try 12 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.410000e+02 : -1.410000e+02\n",
      "Try 13 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 2.130000e+02 : 2.130000e+02\n",
      "Try 14 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.710000e+02 : -1.710000e+02\n",
      "Try 15 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -2.640000e+02 : -2.640000e+02\n",
      "Try 16 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 1.800000e+02 : 1.800000e+02\n",
      "Try 17 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 2.570000e+02 : 2.570000e+02\n",
      "Try 18 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -5.300000e+02 : -5.300000e+02\n",
      "Try 19 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 1.540000e+02 : 1.540000e+02\n",
      "Try 20 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 8.200000e+01 : 8.200000e+01\n",
      "Try 21 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.320000e+02 : -1.320000e+02\n",
      "Try 22 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 2.590000e+02 : 2.590000e+02\n",
      "Try 23 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 4.500000e+01 : 4.500000e+01\n",
      "Try 24 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.300000e+01 : -1.300000e+01\n",
      "Try 25 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 1.020000e+02 : 1.020000e+02\n",
      "Try 26 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -1.900000e+01 : -1.900000e+01\n",
      "Try 27 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -3.120000e+02 : -3.120000e+02\n",
      "Try 28 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 3.330000e+02 : 3.330000e+02\n",
      "Try 29 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 1.580000e+02 : 1.580000e+02\n",
      "Try 30 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 4.400000e+01 : 4.400000e+01\n",
      "Try 31 : 0 / 4096 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : 0.000000e+00 : -8.200000e+01 : -8.200000e+01\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "  kdim = 128*32\n",
    "\n",
    "  test_input_int = torch.randint(-2, 3, (1, 1, kdim), dtype=torch.float16)\n",
    "  test_input_scale = torch.randint(-2, 3, (1, 1, kdim//128), dtype=torch.float16)\n",
    "  test_input = test_input_int * torch.repeat_interleave(test_input_scale, 128, dim=-1)\n",
    "\n",
    "  test_weight_int = torch.randint(-2, 3, (kdim, kdim), dtype=torch.float16)\n",
    "  test_weight_scale = torch.randint(-2, 3, (kdim, kdim//128), dtype=torch.float16)\n",
    "  test_weight = test_weight_int * torch.repeat_interleave(test_weight_scale, 128, dim=-1)\n",
    "\n",
    "  test_ref_output = torch.functional.F.linear(test_input.to(DEV), test_weight.to(DEV), None).cpu()\n",
    "\n",
    "  layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "  origin_layer = Dummy()\n",
    "  origin_layer.weight = test_weight\n",
    "  origin_layer.weight_scale = test_weight_scale\n",
    "  # layer_v2.construct(model.model.layers[layer_num].self_attn.q_proj, kdim)\n",
    "  layer_v2.construct(origin_layer)\n",
    "  layer_v2 = layer_v2.to(DEV)\n",
    "\n",
    "  test_eval_output = layer_v2(test_input.to(DEV), test_input_int.to(DEV), test_input_scale.to(DEV)).cpu()\n",
    "\n",
    "  # err = torch.abs(test_ref_output - test_eval_output).float()/(torch.abs(test_ref_output).float() + 1e-15)\n",
    "  err = torch.abs(test_ref_output - test_eval_output).float()\n",
    "  # err = torch.abs(test_ref_output.float() - test_eval_output.float())/(torch.abs(test_ref_output.float()))\n",
    "  err_cnt = (err > 1e-3).sum() # 0.1% error\n",
    "  err_mean = err.mean()\n",
    "  err_max = err.max()\n",
    "  err_min = err.min()\n",
    "  err_std = err.std()\n",
    "  err_max_index = torch.argmax(err.view(-1))\n",
    "\n",
    "  print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_std:e} : {err_max:e} : {err_min:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with float16 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 0 : 0 / 4096 : 4.115918e-06 : 1.876173e-03 : 2.439880e-02 : 2.435303e-02 : 2.010300e-03 : 4.820983e-04\n",
      "Try 1 : 0 / 4096 : 3.810685e-06 : 1.204819e-03 : 5.065918e-02 : 5.072021e-02 : 1.151318e-03 : 4.832001e-04\n",
      "Try 2 : 0 / 4096 : 3.000197e-06 : 9.191177e-04 : 8.500000e+00 : 8.507812e+00 : 5.360486e-04 : 4.731461e-04\n",
      "Try 3 : 0 / 4096 : 5.505268e-06 : 4.869142e-03 : -3.133774e-03 : -3.149033e-03 : 4.628885e-03 : 4.796144e-04\n",
      "Try 4 : 0 / 4096 : 4.298681e-06 : 9.578544e-04 : 1.274414e-01 : 1.273193e-01 : 8.397230e-04 : 5.051823e-04\n",
      "Try 5 : 0 / 4096 : 5.010760e-06 : 4.615385e-03 : -2.479553e-03 : -2.490997e-03 : 4.522349e-03 : 7.651378e-04\n",
      "Try 6 : 0 / 4096 : 5.563616e-06 : 4.369993e-03 : 1.047516e-02 : 1.052094e-02 : 4.548364e-03 : 4.824219e-04\n",
      "Try 7 : 0 / 4096 : 4.204961e-06 : 1.620746e-03 : -2.824402e-02 : -2.828979e-02 : 1.401413e-03 : 4.710025e-04\n",
      "Try 8 : 0 / 4096 : 4.004637e-06 : 9.416196e-04 : -5.185547e-01 : -5.180664e-01 : 4.929639e-04 : 4.770881e-04\n",
      "Try 9 : 0 / 4096 : 3.088414e-06 : 2.259887e-03 : -1.350403e-02 : -1.353455e-02 : 2.238074e-03 : 5.354084e-04\n",
      "Try 10 : 0 / 4096 : 2.556236e-06 : 8.810573e-04 : 1.773438e+01 : 1.775000e+01 : 4.710401e-04 : 4.710401e-04\n",
      "Try 11 : 0 / 4096 : 4.710179e-06 : 5.823187e-03 : -7.205963e-03 : -7.164001e-03 : 6.129353e-03 : 4.860620e-04\n",
      "Try 12 : 0 / 4096 : 4.562798e-06 : 2.987304e-03 : 5.107880e-03 : 5.092621e-03 : 3.614410e-03 : 6.163089e-04\n",
      "Try 13 : 0 / 4096 : 4.886855e-06 : 1.999001e-03 : -7.633209e-03 : -7.617950e-03 : 2.144143e-03 : 4.758617e-04\n",
      "Try 14 : 0 / 4096 : 5.032095e-06 : 2.386635e-03 : 1.918030e-02 : 1.913452e-02 : 2.343851e-03 : 4.846195e-04\n",
      "Try 15 : 0 / 4096 : 2.705518e-06 : 8.952551e-04 : 4.363281e+00 : 4.359375e+00 : 8.434319e-04 : 4.801903e-04\n",
      "Try 16 : 0 / 4096 : 5.136164e-06 : 1.686341e-03 : -3.619385e-02 : -3.613281e-02 : 2.160348e-03 : 5.200641e-04\n",
      "Try 17 : 0 / 4096 : 5.337945e-06 : 1.652893e-03 : -9.231567e-03 : -9.246826e-03 : 1.456694e-03 : 4.838523e-04\n",
      "Try 18 : 0 / 4096 : 4.173192e-06 : 9.345795e-04 : 2.612305e-01 : 2.609863e-01 : 8.595935e-04 : 4.822415e-04\n",
      "Try 19 : 0 / 4096 : 5.052209e-06 : 5.838641e-03 : -3.593445e-03 : -3.614426e-03 : 5.123889e-03 : 6.848354e-04\n",
      "Try 20 : 0 / 4096 : 5.327841e-06 : 3.120125e-03 : -1.956177e-02 : -1.950073e-02 : 2.868573e-03 : 4.847339e-04\n",
      "Try 21 : 0 / 4096 : 3.244187e-06 : 9.157509e-04 : -1.333008e-01 : -1.334229e-01 : 7.749702e-04 : 4.864178e-04\n",
      "Try 22 : 0 / 4096 : 5.019524e-06 : 3.105590e-03 : -1.228333e-02 : -1.232147e-02 : 2.860824e-03 : 4.755044e-04\n",
      "Try 23 : 0 / 4096 : 9.410653e-06 : 8.450705e-03 : -2.708435e-03 : -2.731323e-03 : 8.549089e-03 : 4.875219e-04\n",
      "Try 24 : 0 / 4096 : 6.765239e-06 : 7.438895e-03 : -7.179260e-03 : -7.232666e-03 : 7.719241e-03 : 4.804931e-04\n",
      "Try 25 : 0 / 4096 : 4.009430e-06 : 1.390821e-03 : 2.194214e-02 : 2.197266e-02 : 1.171452e-03 : 4.825397e-04\n",
      "Try 26 : 0 / 4096 : 2.573421e-06 : 9.372071e-04 : -2.604980e-01 : -2.602539e-01 : 7.562181e-04 : 4.751779e-04\n",
      "Try 27 : 0 / 4096 : 5.185002e-06 : 2.962085e-03 : -1.287842e-02 : -1.291656e-02 : 3.020478e-03 : 4.802242e-04\n",
      "Try 28 : 0 / 4096 : 4.162602e-06 : 1.943635e-03 : -7.850647e-03 : -7.835388e-03 : 2.609372e-03 : 7.775642e-04\n",
      "Try 29 : 0 / 4096 : 4.599007e-06 : 5.025126e-03 : 2.277374e-03 : 2.288818e-03 : 5.595607e-03 : 5.985998e-04\n",
      "Try 30 : 0 / 4096 : 5.415728e-06 : 2.076125e-03 : -2.204895e-02 : -2.209473e-02 : 1.805908e-03 : 4.733199e-04\n",
      "Try 31 : 0 / 4096 : 3.588417e-06 : 9.578544e-04 : -2.039062e+00 : -2.037109e+00 : 1.061863e-03 : 4.838094e-04\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "  kdim = 128*32\n",
    "\n",
    "  test_input_int = torch.randint(-1, 2, (1, 1, kdim), dtype=torch.float16)\n",
    "  test_input_scale = torch.randn((1, 1, kdim//128), dtype=torch.float16)\n",
    "  test_input = test_input_int * torch.repeat_interleave(test_input_scale, 128, dim=-1)\n",
    "\n",
    "  test_weight_int = torch.randint(-1, 2, (kdim, kdim), dtype=torch.float16)\n",
    "  test_weight_scale = torch.randn((kdim, kdim//128), dtype=torch.float16)\n",
    "  test_weight = test_weight_int * torch.repeat_interleave(test_weight_scale, 128, dim=-1)\n",
    "\n",
    "  test_golden_ref_output = torch.functional.F.linear(test_input.to(DEV).to(torch.float64), test_weight.to(DEV).to(torch.float64), None).cpu()\n",
    "  test_ref_output = torch.functional.F.linear(test_input.to(DEV), test_weight.to(DEV), None).cpu()\n",
    "\n",
    "  layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "  origin_layer = Dummy()\n",
    "  origin_layer.weight = test_weight\n",
    "  origin_layer.weight_scale = test_weight_scale\n",
    "  # layer_v2.construct(model.model.layers[layer_num].self_attn.q_proj, kdim)\n",
    "  layer_v2.construct(origin_layer)\n",
    "  layer_v2 = layer_v2.to(DEV)\n",
    "\n",
    "  test_eval_output = layer_v2(test_input.to(DEV), test_input_int.to(DEV), test_input_scale.to(DEV)).cpu()\n",
    "\n",
    "  err = torch.abs(test_ref_output - test_eval_output).float()/(torch.abs(test_ref_output).float() + 1e-15)\n",
    "  # err = torch.abs(test_ref_output - test_eval_output).float()\n",
    "  # err = torch.abs(test_ref_output.float() - test_eval_output.float())/(torch.abs(test_ref_output.float()))\n",
    "  err_cnt = (err > 1e-2).sum() # 0.1% error\n",
    "  err_mean = err.mean()\n",
    "  err_max = err.max()\n",
    "  err_min = err.min()\n",
    "  err_std = err.std()\n",
    "  err_max_index = torch.argmax(err.view(-1))\n",
    "\n",
    "  err_ref = torch.abs(test_golden_ref_output - test_ref_output).float()/(torch.abs(test_golden_ref_output).float() + 1e-15)\n",
    "  err_ref_cnt = (err_ref > 1e-2).sum()\n",
    "  err_ref_max = err_ref.max()\n",
    "  err_ref_mean = err_ref.mean()\n",
    "\n",
    "  err_eval = torch.abs(test_golden_ref_output - test_eval_output).float()/(torch.abs(test_golden_ref_output).float() + 1e-15)\n",
    "  err_eval_cnt = (err_eval > 1e-2).sum()\n",
    "  err_eval_max = err_eval.max()\n",
    "  err_eval_mean = err_eval.mean()\n",
    "\n",
    "  # print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_std:e} : {err_max:e} : {err_min:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")\n",
    "  print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_max:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e} : {err_ref_max:e} : {err_eval_max:e}\")\n",
    "  # print(f\"max index : {err_max_index} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "  kdim = 128*32\n",
    "\n",
    "  test_input_int = torch.randint(-1, 2, (1, 1, kdim), dtype=torch.float16)\n",
    "  test_input_scale = torch.randn((1, 1, kdim//128), dtype=torch.float16)\n",
    "  test_input = test_input_int * torch.repeat_interleave(test_input_scale, 128, dim=-1)\n",
    "\n",
    "  test_weight_int = torch.randint(-1, 2, (kdim, kdim), dtype=torch.float16)\n",
    "  test_weight_scale = torch.randn((kdim, kdim//128), dtype=torch.float16)\n",
    "  test_weight = test_weight_int * torch.repeat_interleave(test_weight_scale, 128, dim=-1)\n",
    "\n",
    "  test_golden_ref_output = torch.functional.F.linear(test_input.to(DEV).to(torch.float64), test_weight.to(DEV).to(torch.float64), None).cpu()\n",
    "  test_ref_output = torch.functional.F.linear(test_input.to(DEV), test_weight.to(DEV), None).cpu()\n",
    "\n",
    "  layer_v2 = qLinearLayer.QLinearLayerACIM()\n",
    "  origin_layer = Dummy()\n",
    "  origin_layer.weight = test_weight\n",
    "  origin_layer.weight_scale = test_weight_scale\n",
    "  # layer_v2.construct(model.model.layers[layer_num].self_attn.q_proj, kdim)\n",
    "  layer_v2.construct(origin_layer)\n",
    "  layer_v2 = layer_v2.to(DEV)\n",
    "\n",
    "  test_eval_output = layer_v2(test_input.to(DEV), test_input_int.to(DEV), test_input_scale.to(DEV)).cpu()\n",
    "\n",
    "  err = torch.abs(test_ref_output - test_eval_output).float()/(torch.abs(test_ref_output).float() + 1e-15)\n",
    "  # err = torch.abs(test_ref_output - test_eval_output).float()\n",
    "  # err = torch.abs(test_ref_output.float() - test_eval_output.float())/(torch.abs(test_ref_output.float()))\n",
    "  err_cnt = (err > 1e-2).sum() # 0.1% error\n",
    "  err_mean = err.mean()\n",
    "  err_max = err.max()\n",
    "  err_min = err.min()\n",
    "  err_std = err.std()\n",
    "  err_max_index = torch.argmax(err.view(-1))\n",
    "\n",
    "  err_ref = torch.abs(test_golden_ref_output - test_ref_output).float()/(torch.abs(test_golden_ref_output).float() + 1e-15)\n",
    "  err_ref_cnt = (err_ref > 1e-2).sum()\n",
    "  err_ref_max = err_ref.max()\n",
    "  err_ref_mean = err_ref.mean()\n",
    "\n",
    "  err_eval = torch.abs(test_golden_ref_output - test_eval_output).float()/(torch.abs(test_golden_ref_output).float() + 1e-15)\n",
    "  err_eval_cnt = (err_eval > 1e-2).sum()\n",
    "  err_eval_max = err_eval.max()\n",
    "  err_eval_mean = err_eval.mean()\n",
    "\n",
    "  # print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_std:e} : {err_max:e} : {err_min:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")\n",
    "  print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_max:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e} : {err_ref_max:e} : {err_eval_max:e}\")\n",
    "  # print(f\"max index : {err_max_index} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with FP 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 0 : 93 / 4096 : 2.864564e-03 : 1.628227e+00 : -2.120399e-02 : 1.332092e-02\n",
      "Try 1 : 114 / 4096 : 3.384615e-03 : 4.323081e+00 : 7.441521e-03 : 3.961182e-02\n",
      "Try 2 : 112 / 4096 : 2.548672e-03 : 5.077909e-01 : 6.132698e-02 : 9.246826e-02\n",
      "Try 3 : 107 / 4096 : 2.765086e-03 : 2.833956e+00 : -5.869865e-03 : 1.076508e-02\n",
      "Try 4 : 87 / 4096 : 2.196012e-03 : 1.068597e+00 : -2.381516e-02 : 1.633644e-03\n",
      "Try 5 : 104 / 4096 : 2.368168e-03 : 6.010029e-01 : 9.889603e-02 : 3.945923e-02\n",
      "Try 6 : 102 / 4096 : 4.350397e-03 : 8.408163e+00 : 7.476807e-04 : -5.538940e-03\n",
      "Try 7 : 84 / 4096 : 1.843648e-03 : 7.302278e-01 : 1.314354e-02 : 3.545761e-03\n",
      "Try 8 : 85 / 4096 : 1.817901e-02 : 3.335714e+01 : -1.281738e-03 : -4.403687e-02\n",
      "Try 9 : 105 / 4096 : 5.060241e-03 : 1.144288e+01 : 1.085281e-03 : 1.350403e-02\n",
      "Try 10 : 87 / 4096 : 3.541557e-03 : 5.241799e+00 : -4.070282e-03 : -2.540588e-02\n",
      "Try 11 : 86 / 4096 : 1.454487e-02 : 4.985994e+01 : -2.927780e-04 : 1.430511e-02\n",
      "Try 12 : 97 / 4096 : 2.967565e-03 : 2.722442e+00 : -4.226208e-03 : -1.573181e-02\n",
      "Try 13 : 112 / 4096 : 2.596082e-03 : 1.018785e+00 : -2.122116e-02 : 3.986359e-04\n",
      "Try 14 : 107 / 4096 : 2.481115e-03 : 5.887039e-01 : 2.944756e-02 : 4.678345e-02\n",
      "Try 15 : 97 / 4096 : 2.016367e-03 : 4.551352e-01 : -6.115532e-02 : -8.898926e-02\n",
      "Try 16 : 103 / 4096 : 2.950502e-03 : 1.422060e+00 : -1.843357e-02 : -4.464722e-02\n",
      "Try 17 : 110 / 4096 : 2.683316e-03 : 1.673145e+00 : -2.159119e-03 : -5.771637e-03\n",
      "Try 18 : 108 / 4096 : 1.991094e-03 : 3.058259e-01 : -5.565643e-02 : -3.863525e-02\n",
      "Try 19 : 98 / 4096 : 1.837769e-03 : 2.077836e-01 : -4.626465e-02 : -3.665161e-02\n",
      "Try 20 : 94 / 4096 : 1.700786e-03 : 2.659519e-01 : 1.031876e-01 : 7.574463e-02\n",
      "Try 21 : 90 / 4096 : 1.882517e-03 : 5.471275e-01 : -6.799316e-02 : -3.079224e-02\n",
      "Try 22 : 111 / 4096 : 2.253660e-03 : 3.675807e-01 : 2.331696e-01 : 1.474609e-01\n",
      "Try 23 : 123 / 4096 : 3.100296e-03 : 2.131330e+00 : -2.222061e-02 : -6.958008e-02\n",
      "Try 24 : 104 / 4096 : 3.017965e-03 : 2.201327e+00 : -2.069092e-02 : 2.485657e-02\n",
      "Try 25 : 87 / 4096 : 2.136576e-03 : 1.338401e+00 : -7.778168e-03 : -1.818848e-02\n",
      "Try 26 : 96 / 4096 : 2.283275e-03 : 1.833144e+00 : 2.388239e-02 : -1.989746e-02\n",
      "Try 27 : 119 / 4096 : 2.149804e-03 : 5.091287e-01 : 3.207207e-02 : 4.840088e-02\n",
      "Try 28 : 104 / 4096 : 3.533258e-03 : 2.745128e+00 : -2.206898e-02 : 3.851318e-02\n",
      "Try 29 : 109 / 4096 : 1.962432e-03 : 2.438404e-01 : 8.321953e-02 : 6.292725e-02\n",
      "Try 30 : 97 / 4096 : 2.495255e-03 : 1.192354e+00 : 1.257324e-02 : -2.418518e-03\n",
      "Try 31 : 110 / 4096 : 2.789142e-03 : 9.590091e-01 : -2.975655e-02 : -1.219749e-03\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "  kdim = 128*32\n",
    "\n",
    "  test_input_int = torch.randint(-1, 2, (1, 1, kdim), dtype=torch.float32)\n",
    "  test_input_scale = torch.randn((1, 1, kdim//128), dtype=torch.float32)\n",
    "  test_input = test_input_int * torch.repeat_interleave(test_input_scale, 128, dim=-1)\n",
    "\n",
    "  test_weight_int = torch.randint(-1, 2, (kdim, kdim), dtype=torch.float32)\n",
    "  test_weight_scale = torch.randn((kdim, kdim//128), dtype=torch.float32)\n",
    "  test_weight = test_weight_int * torch.repeat_interleave(test_weight_scale, 128, dim=-1)\n",
    "\n",
    "  test_ref_output = torch.functional.F.linear(test_input.to(DEV), test_weight.to(DEV), None).cpu()\n",
    "\n",
    "  layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "  origin_layer = Dummy()\n",
    "  origin_layer.weight = test_weight\n",
    "  origin_layer.weight_scale = test_weight_scale\n",
    "  # layer_v2.construct(model.model.layers[layer_num].self_attn.q_proj, kdim)\n",
    "  layer_v2.construct(origin_layer)\n",
    "  layer_v2 = layer_v2.to(DEV)\n",
    "\n",
    "  test_eval_output = layer_v2(test_input.to(DEV), test_input_int.to(DEV), test_input_scale.to(DEV)).cpu()\n",
    "\n",
    "  err = torch.abs(test_ref_output - test_eval_output).float()/(torch.abs(test_ref_output).float() + 1e-15)\n",
    "  # err = torch.abs(test_ref_output - test_eval_output).float()\n",
    "  # err = torch.abs(test_ref_output.float() - test_eval_output.float())/(torch.abs(test_ref_output.float()))\n",
    "  err_cnt = (err > 1e-2).sum() # 0.1% error\n",
    "  err_mean = err.mean()\n",
    "  err_max = err.max()\n",
    "  err_min = err.min()\n",
    "  err_std = err.std()\n",
    "  err_max_index = torch.argmax(err.view(-1))\n",
    "\n",
    "  # print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_std:e} : {err_max:e} : {err_min:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")\n",
    "  print(f\"Try {i} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_max:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")\n",
    "  # print(f\"max index : {err_max_index} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 : 594 / 4096 : 5.433440e-03 : 2.523691e-01 : 1.604636e+01 : 0.000000e+00 : 7.200241e-05 : -1.083374e-03\n",
      "Layer 1 : 622 / 4096 : 1.607181e-03 : 1.943378e-02 : 8.572448e-01 : 0.000000e+00 : -1.336098e-03 : -2.481461e-03\n",
      "Layer 2 : 649 / 4096 : 1.886206e-03 : 3.897713e-02 : 2.420800e+00 : 0.000000e+00 : 2.384186e-03 : -3.387451e-03\n",
      "Layer 3 : 637 / 4096 : 2.997441e-03 : 1.040268e-01 : 6.594501e+00 : 0.000000e+00 : -2.775192e-04 : 1.552582e-03\n",
      "Layer 4 : 609 / 4096 : 1.126373e-03 : 8.851678e-03 : 3.536730e-01 : 0.000000e+00 : 3.219604e-03 : 2.080917e-03\n",
      "Layer 5 : 656 / 4096 : 1.756979e-03 : 3.308890e-02 : 1.751302e+00 : 0.000000e+00 : -1.464844e-03 : 1.100540e-03\n",
      "Layer 6 : 662 / 4096 : 1.193928e-03 : 8.359358e-03 : 3.156168e-01 : 0.000000e+00 : -2.906799e-03 : -1.989365e-03\n",
      "Layer 7 : 636 / 4096 : 2.473832e-03 : 5.294995e-02 : 2.810438e+00 : 0.000000e+00 : 3.974438e-04 : 1.514435e-03\n",
      "Layer 8 : 602 / 4096 : 1.034629e-03 : 6.044023e-03 : 2.339956e-01 : 0.000000e+00 : -3.456116e-03 : -2.647400e-03\n",
      "Layer 9 : 602 / 4096 : 1.295313e-03 : 1.550764e-02 : 7.690920e-01 : 0.000000e+00 : -3.171921e-03 : -5.611420e-03\n",
      "Layer 10 : 672 / 4096 : 1.515840e-03 : 2.116747e-02 : 1.224038e+00 : 0.000000e+00 : 3.618240e-03 : -8.106232e-04\n",
      "Layer 11 : 633 / 4096 : 1.580145e-03 : 2.744790e-02 : 1.583987e+00 : 0.000000e+00 : -1.822472e-03 : 1.064301e-03\n",
      "Layer 12 : 643 / 4096 : 1.435253e-03 : 1.568600e-02 : 8.445610e-01 : 0.000000e+00 : -1.727104e-03 : -2.684593e-04\n",
      "Layer 13 : 679 / 4096 : 2.315779e-03 : 4.793189e-02 : 2.771831e+00 : 0.000000e+00 : -1.692772e-04 : -6.384850e-04\n",
      "Layer 14 : 628 / 4096 : 2.095387e-03 : 3.575997e-02 : 1.935383e+00 : 0.000000e+00 : 1.520157e-03 : -1.421928e-03\n",
      "Layer 15 : 614 / 4096 : 1.238216e-03 : 1.518522e-02 : 9.195141e-01 : 0.000000e+00 : -1.255989e-03 : -2.410889e-03\n",
      "Layer 16 : 661 / 4096 : 1.156044e-03 : 8.567823e-03 : 4.227139e-01 : 0.000000e+00 : 6.465912e-03 : 3.732681e-03\n",
      "Layer 17 : 654 / 4096 : 1.765387e-03 : 2.151911e-02 : 9.059973e-01 : 0.000000e+00 : 7.076263e-04 : 6.651878e-05\n",
      "Layer 18 : 609 / 4096 : 1.034407e-03 : 7.146471e-03 : 3.246269e-01 : 0.000000e+00 : 1.022339e-02 : 1.354218e-02\n",
      "Layer 19 : 641 / 4096 : 1.433341e-03 : 1.283587e-02 : 5.909091e-01 : 0.000000e+00 : -2.853394e-03 : -4.539490e-03\n",
      "Layer 20 : 558 / 4096 : 3.734552e-02 : 2.326817e+00 : 1.489167e+02 : 0.000000e+00 : 5.722046e-06 : 8.578300e-04\n",
      "Layer 21 : 643 / 4096 : 4.883809e-03 : 1.475080e-01 : 8.813581e+00 : 0.000000e+00 : 8.952618e-05 : -6.995201e-04\n",
      "Layer 22 : 600 / 4096 : 3.127739e-03 : 8.710182e-02 : 4.752540e+00 : 0.000000e+00 : -1.595497e-03 : -9.178162e-03\n",
      "Layer 23 : 631 / 4096 : 1.935420e-03 : 3.802509e-02 : 1.809647e+00 : 0.000000e+00 : -4.596710e-04 : 3.721714e-04\n",
      "Layer 24 : 620 / 4096 : 3.467354e-03 : 1.150813e-01 : 5.260417e+00 : 0.000000e+00 : -4.577637e-04 : 1.950264e-03\n",
      "Layer 25 : 637 / 4096 : 9.883138e-04 : 4.859523e-03 : 1.550343e-01 : 0.000000e+00 : 1.333618e-02 : 1.540375e-02\n",
      "Layer 26 : 623 / 4096 : 1.548431e-03 : 1.911190e-02 : 1.020213e+00 : 0.000000e+00 : 8.492470e-04 : 1.715660e-03\n",
      "Layer 27 : 648 / 4096 : 1.514447e-03 : 1.510029e-02 : 6.029648e-01 : 0.000000e+00 : 4.760742e-03 : 1.890182e-03\n",
      "Layer 28 : 624 / 4096 : 2.136649e-03 : 2.888189e-02 : 1.231444e+00 : 0.000000e+00 : -5.974770e-04 : -1.333237e-03\n",
      "Layer 29 : 624 / 4096 : 3.199523e-03 : 8.307087e-02 : 4.499564e+00 : 0.000000e+00 : -2.734661e-04 : -1.503944e-03\n",
      "Layer 30 : 628 / 4096 : 1.499572e-03 : 2.737338e-02 : 1.709534e+00 : 0.000000e+00 : -8.602142e-04 : -2.330780e-03\n",
      "Layer 31 : 690 / 4096 : 1.732240e-03 : 2.987723e-02 : 1.835932e+00 : 0.000000e+00 : 1.284599e-03 : 3.643036e-03\n"
     ]
    }
   ],
   "source": [
    "kdim = 128*32\n",
    "for i in range(32):\n",
    "  layer_num = i\n",
    "  test_input_int = torch.randint(-8, 8, (1, 1, kdim), dtype=torch.float16)\n",
    "  test_input_scale = torch.randn(1, 1, kdim//128, dtype=torch.float16)\n",
    "  test_input = test_input_int * torch.repeat_interleave(test_input_scale, 128, dim=2)\n",
    "  test_weight = model.model.layers[layer_num].self_attn.q_proj.weight[:, :kdim]\n",
    "  # test_ref_output = torch.functional.F.linear(test_input.to(DEV).float(), test_weight.to(DEV).float(), None).cpu().to(torch.float16)\n",
    "  test_ref_output = torch.functional.F.linear(test_input.to(DEV), test_weight.to(DEV), None).cpu()\n",
    "\n",
    "  layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "  # layer_v2.construct(model.model.layers[layer_num].self_attn.q_proj, kdim)\n",
    "  layer_v2.construct(model.model.layers[layer_num].self_attn.q_proj)\n",
    "  layer_v2 = layer_v2.to(DEV)\n",
    "\n",
    "  test_eval_output = layer_v2(test_input.to(DEV), test_input_int.to(DEV), test_input_scale.to(DEV)).cpu()\n",
    "\n",
    "  # err = torch.abs(test_ref_output - test_eval_output).float()/(torch.abs(test_ref_output).float() + 1e-15)\n",
    "  # err = torch.abs(test_ref_output - test_eval_output).float()\n",
    "  err = torch.abs(test_ref_output.float() - test_eval_output.float())/(torch.abs(test_ref_output.float()))\n",
    "  err_cnt = (err > 1e-3).sum() # 0.1% error\n",
    "  err_mean = err.mean()\n",
    "  err_max = err.max()\n",
    "  err_min = err.min()\n",
    "  err_std = err.std()\n",
    "  err_max_index = torch.argmax(err.view(-1))\n",
    "  print(f\"Layer {layer_num} : {err_cnt} / {err.numel()} : {err_mean:e} : {err_std:e} : {err_max:e} : {err_min:e} : {test_ref_output.view(-1)[err_max_index]:e} : {test_eval_output.view(-1)[err_max_index]:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.6953125000, dtype=torch.float16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_input[0,0,:] * model.model.layers[0].self_attn.q_proj.weight[3056, :].T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5244140625, -2.0800781250,  0.3061523438, -2.1523437500,\n",
      "         5.0351562500,  7.2773437500,  1.6943359375,  0.7119140625,\n",
      "         4.9531250000,  0.2219238281, -3.8164062500, -4.0703125000,\n",
      "         5.4179687500, -8.6718750000,  0.2008056641,  1.5488281250],\n",
      "       dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0071716309, -0.0171661377,  0.0052490234, -0.0126266479,\n",
      "         0.0251922607,  0.0402221680,  0.0084991455,  0.0085220337,\n",
      "         0.0194549561, -0.0055122375, -0.0195770264, -0.0305786133,\n",
      "         0.0326232910, -0.0509033203, -0.0009284019,  0.0083770752],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(test_ref_output.flatten()[0:16])\n",
    "print(test_eval_output.flatten()[0:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0936279297, -0.0312042236,  0.0000000000,  0.0312042236],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<Unique2Backward0>)\n",
      "tensor(0.0312042236, device='cuda:0', dtype=torch.float16)\n",
      "tensor(0)\n",
      "tensor([-0.0936279297, -0.0312042236,  0.0000000000,  0.0312042236],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<Unique2Backward0>)\n",
      "tensor([-0.0936279297, -0.0312042236,  0.0000000000,  0.0312042236],\n",
      "       dtype=torch.float16, grad_fn=<Unique2Backward0>)\n"
     ]
    }
   ],
   "source": [
    "# print(test_weight[0, 0:128].unique())\n",
    "# print(model.model.layers[layer_num].self_attn.q_proj.weight_scale[0,0].to(torch.float16))\n",
    "\n",
    "# test_weight_int = torch.round(test_weight[0, 0:128].cpu()/model.model.layers[layer_num].self_attn.q_proj.weight_scale[0,0].cpu().to(torch.float16))\n",
    "\n",
    "# test_weight_dq = test_weight_int * model.model.layers[layer_num].self_attn.q_proj.weight_scale[0,0].cpu().to(torch.float16)\n",
    "# print(((test_weight_dq-test_weight[0, 0:128].cpu())>1e-5).sum())\n",
    "\n",
    "# print(test_weight[0, 0:128].unique())\n",
    "# print(test_weight_dq.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_layers = {}\n",
    "for name, m in model.model.named_modules():\n",
    "    if isinstance(m, qLinearLayer.QLinearLayer):\n",
    "      layer_v2 = qLinearLayer.QLinearLayerV2()\n",
    "      layer_v2.args = m.args\n",
    "      layer_v2.weight = m.weight\n",
    "      layer_v2.bias = m.bias\n",
    "      changed_layers[name] = layer_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in changed_layers.items():\n",
    "    analyze.set_nested_attr(model.model, name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x QLlamaDecoderLayer(\n",
      "        (self_attn): QLlamaAttention(\n",
      "          (q_proj): QLinearLayerV2()\n",
      "          (k_proj): QLinearLayerV2()\n",
      "          (v_proj): QLinearLayerV2()\n",
      "          (o_proj): QLinearLayerV2()\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (act_quant): Quantizer()\n",
      "          (v_quant): Quantizer()\n",
      "          (k_quant): Quantizer()\n",
      "        )\n",
      "        (mlp): QLlamaMLP(\n",
      "          (gate_proj): QLinearLayerV2()\n",
      "          (down_proj): QLinearLayerV2()\n",
      "          (up_proj): QLinearLayerV2()\n",
      "          (act_fn): SiLU()\n",
      "          (act_quant): Quantizer()\n",
      "        )\n",
      "        (input_layernorm): QLlamaRMSNorm(\n",
      "          (originalNorm): LlamaRMSNorm()\n",
      "          (act_quant): Quantizer()\n",
      "        )\n",
      "        (post_attention_layernorm): QLlamaRMSNorm(\n",
      "          (originalNorm): LlamaRMSNorm()\n",
      "          (act_quant): Quantizer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wikitext2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [1:20:27<00:00, 150.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetResult,wikitext2,nan\n"
     ]
    }
   ],
   "source": [
    "datasets = ['wikitext2']\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=\"./llama2-7b\", seqlen=model.seqlen\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluating {dataset} ...\")\n",
    "    ppl = llama_eval(model, testloader, DEV)\n",
    "\n",
    "    print(f\"targetResult,{dataset},{ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  32000\n",
      "map layer 0 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 1 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 2 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 3 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 4 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 5 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 6 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 7 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 8 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 9 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 10 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 11 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 12 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 13 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 14 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 15 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 16 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 17 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 18 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 19 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 20 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 21 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 22 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 23 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 24 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 25 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 26 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 27 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 28 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 29 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map layer 30 to gpu 2, [(0, 24576, 14462), (1, 24576, 14086), (2, 24576, 13658), (3, 24576, 13664)]\n",
      "map last layer 31 to gpu 2\n",
      "Selected Tasks: ['winogrande']\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1018/2534 [5:43:46<8:31:56, 20.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 190\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected Tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m task_dict \u001b[38;5;241m=\u001b[39m lm_tasks\u001b[38;5;241m.\u001b[39mget_task_dict(task_names)\n\u001b[0;32m--> 190\u001b[0m t_results \u001b[38;5;241m=\u001b[39m \u001b[43mlm_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fewshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_eval_num_fewshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_eval_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_eval_limit\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m results\u001b[38;5;241m.\u001b[39mupdate(t_results)\n\u001b[1;32m    197\u001b[0m pprint(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/utils.py:161\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/evaluator.py:247\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(lm, task_dict, provide_description, num_fewshot, limit, bootstrap_iters, description_dict, decontamination_ngrams_path)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reqtype, reqs \u001b[38;5;129;01min\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# TODO: right now, this code runs multiple separate LM requests for multiple Requests differing\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#       only in index. We could implement some kind of caching, but that would be more of a band-aid\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m#       solution. we could also implement some kind of auto-grouping here;\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m#       they should end up next to each other.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning\u001b[39m\u001b[38;5;124m\"\u001b[39m, reqtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m     resps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreqs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     resps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    249\u001b[0m         x \u001b[38;5;28;01mif\u001b[39;00m req\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x[req\u001b[38;5;241m.\u001b[39mindex] \u001b[38;5;28;01mfor\u001b[39;00m x, req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, reqs)\n\u001b[1;32m    250\u001b[0m     ]\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resp, (i, task_name, doc, doc_id) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, requests_origin[reqtype]):\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/base.py:185\u001b[0m, in \u001b[0;36mBaseLM.loglikelihood\u001b[0;34m(self, requests)\u001b[0m\n\u001b[1;32m    181\u001b[0m     continuation_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_encode(continuation)\n\u001b[1;32m    183\u001b[0m     new_reqs\u001b[38;5;241m.\u001b[39mappend(((context, continuation), context_enc, continuation_enc))\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loglikelihood_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_reqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/lm_eval/base.py:295\u001b[0m, in \u001b[0;36mBaseLM._loglikelihood_tokens\u001b[0;34m(self, requests, disable_tqdm)\u001b[0m\n\u001b[1;32m    291\u001b[0m     inplens\u001b[38;5;241m.\u001b[39mappend(inplen)\n\u001b[1;32m    293\u001b[0m batched_inps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inps, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [batch, padding_length\u001b[39;00m\n\u001b[1;32m    294\u001b[0m multi_logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inps\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    296\u001b[0m )\u001b[38;5;241m.\u001b[39mcpu()  \u001b[38;5;66;03m# [batch, padding_length, vocab]\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (cache_key, _, _), logits, inp, inplen, cont_toks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    299\u001b[0m     chunk, multi_logits, inps, inplens, cont_toks_list\n\u001b[1;32m    300\u001b[0m ):\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# Slice to original seq length\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     contlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cont_toks)\n",
      "File \u001b[0;32m~/project/Atom/./model/LMClass.py:108\u001b[0m, in \u001b[0;36mLMClass._model_call\u001b[0;34m(self, inps)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03minps: a torch tensor of shape [batch, sequence]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mthe size of sequence may vary from call to call\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mreturns: a torch tensor of shape [batch, sequence, vocab] with the\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mlogits returned from the model\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1196\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         cache_position,\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/Atom/./model/qLlamaLayer.py:116\u001b[0m, in \u001b[0;36mQLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, cache_position)\u001b[0m\n\u001b[1;32m    114\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    115\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 116\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    119\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/Atom/./model/qLlamaLayer.py:351\u001b[0m, in \u001b[0;36mQLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# Quantize the activations and feed into down_proj\u001b[39;00m\n\u001b[1;32m    350\u001b[0m tmpResult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_quant(tmpResult)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmpResult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/Atom/./model/qLinearLayer.py:103\u001b[0m, in \u001b[0;36mQLinearLayerV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     out_shape \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 103\u001b[0m     weight_scales \u001b[38;5;241m=\u001b[39m analyze\u001b[38;5;241m.\u001b[39mget_weight_quant_scale(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    104\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(out_shape, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='LlaMa model to load; pass location of hugginface converted checkpoint.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, \n",
    "    help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "# Quantization Method\n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing weight; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--abits', type=int, default=16, choices=[2, 3, 4, 8, 16],\n",
    "    help='#bits to use for quantizing activation; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--exponential', action='store_true',\n",
    "    help='Whether to use exponent-only for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization. Default is asymmetric.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static', action='store_true',\n",
    "    help='Whether to perform static quantization (For activtions). Default is dynamic. (Deprecated in Atom)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight_group_size', type=int, default=0, choices=[0, 32, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing weights. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument( #- ??\n",
    "    '--weight_channel_group', type=int, default=1,\n",
    "    help='Group size of channels that will quantize together. (only for weights now)'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_group_size', type=int, default=0, choices=[0, 64, 128, 256, 384, 768],\n",
    "    help='Group size when quantizing activations. Using 128 as default quantization group.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--reorder', action='store_true',\n",
    "    help='Whether to keep salient weight unquantized.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act_sort_metric', type=str, default='hessian', choices=['abs_mean', 'hessian'],\n",
    "    help='The metric used to sort the activations.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper', type=int, default=0,\n",
    "    help='Group size to keep outliers.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--keeper_precision', type=int, default=0, choices=[0, 1, 2, 3],\n",
    "    help='Precision to keep outliers. 0 for FP16; 1 for E5M2; 2 for E4M3; 3 for INT8 Quant.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--cache_index', action='store_true',\n",
    "    help='Whether to use cached reorder index'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tiling', type=int, default=0, choices=[0, 16],\n",
    "    help='Tile-wise quantization granularity (Deprecated in Atom).'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_cache', action='store_true',\n",
    "    help='Whether to quant KV_Cache'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--use_gptq', action='store_true',\n",
    "    help='Whether to use GPTQ for weight quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--a_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for activation quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--w_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for weight quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--kv_clip_ratio', type=float, default=1.0,\n",
    "    help='Clip ratio for kv cache quantization. new_max = max * clip_ratio'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_ppl\", action=\"store_true\",\n",
    "    help='Whether to evaluate perplexity.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_common_sense\", action=\"store_true\",\n",
    "    help='Whether to evaluate zero-shot accuray on commonsense reasoning tasks.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multigpu\", action=\"store_true\", \n",
    "    help=\"at eval, map model to multiple gpus\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_num_fewshot\", type=int, default=0, \n",
    "    help=\"Number of shots in lm evaluation. Default is 0 for zero-shot.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lm_eval_limit\", type=int, default=-1, \n",
    "    help=\"Limit the number of examples in lm evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_dir', type=str, default='./saved',\n",
    "    help='Path to store the reordering indices and quantized weights.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--quant_type', type=str, default='int', choices=['int', 'fp'],\n",
    "    help='Determine the mapped data format by quant_type + n_bits. e.g. int8, fp4.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_model', action=\"store_true\", default=True,\n",
    "    help='Whether to save the quantized model.'\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\n",
    "  args = [\n",
    "    \"/root/project/Atom/llama2-7b\",\n",
    "    \"wikitext2\",\n",
    "    \"--wbits\", \"4\", \"--abits\", \"4\", \"--a_sym\", \"--w_sym\", \"--save_model\",\n",
    "    \"--act_group_size\", \"128\", \"--weight_group_size\", \"128\", \"--weight_channel_group\", \"2\",\n",
    "    \"--reorder\", \"--act_sort_metric\", \"hessian\", \"--cache_index\",\n",
    "    \"--a_clip_ratio\", \"0.9\", \"--w_clip_ratio\", \"0.85\", \"--kv_clip_ratio\", \"1.0\",\n",
    "    \"--keeper\", \"128\", \"--keeper_precision\", \"3\", \"--kv_cache\", \"--use_gptq\",\n",
    "    \"--eval_common_sense\", \"--lm_eval_limit\", \"-1\", \"--multigpu\"\n",
    "  ]\n",
    ")\n",
    "\n",
    "lm = LMClass(args, model)\n",
    "lm.seqlen = 2048\n",
    "lm.model.eval()\n",
    "for param in lm.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if args.multigpu:\n",
    "    if (\"llama\" in args.model.lower()) or (\"mixtral\" in args.model.lower()):\n",
    "        map_layers_to_multi_gpus(lm.model.model.layers)\n",
    "        input_device = lm.model.model.layers[0].device\n",
    "        output_device = lm.model.model.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.embed_tokens.to(input_device)\n",
    "        lm.model.model.norm.to(output_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "    elif \"opt\" in args.model.lower():\n",
    "        map_layers_to_multi_gpus(lm.model.model.decoder.layers)\n",
    "        input_device = lm.model.model.decoder.layers[0].device\n",
    "        output_device = lm.model.model.decoder.layers[-1].device\n",
    "        assert input_device == output_device\n",
    "        lm._device = input_device\n",
    "        lm.model.model.decoder.embed_tokens.to(input_device)\n",
    "        lm.model.model.decoder.embed_positions.to(input_device)\n",
    "        lm.model.model.decoder.final_layer_norm.to(input_device)\n",
    "        lm.model.lm_head.to(output_device)\n",
    "else:\n",
    "    lm._device = DEV\n",
    "    lm.model = lm.model.to(lm.device)\n",
    "\n",
    "results = {}\n",
    "# tasks_str = \"piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande\"\n",
    "# tasks_str = \"hellaswag,winogrande\"\n",
    "# tasks_str = \"hellaswag\"\n",
    "tasks_str = \"winogrande\"\n",
    "task_names = pattern_match(tasks_str.split(\",\"), lm_tasks.ALL_TASKS)\n",
    "print(f\"Selected Tasks: {task_names}\")\n",
    "\n",
    "task_dict = lm_tasks.get_task_dict(task_names)\n",
    "t_results = lm_evaluator.evaluate(\n",
    "    lm,\n",
    "    task_dict,\n",
    "    num_fewshot=args.lm_eval_num_fewshot,\n",
    "    limit=None if args.lm_eval_limit == -1 else args.lm_eval_limit\n",
    ")\n",
    "results.update(t_results)\n",
    "pprint(results)\n",
    "\n",
    "results_dict = results['results']\n",
    "for task_name in tasks_str.split(','):\n",
    "    if task_name in ['piqa', 'arc_easy', 'arc_challenge', 'hellaswag']:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc_norm']*100:.2f}\")\n",
    "    else:\n",
    "        print(f\"INFO {task_name} : {results_dict[task_name]['acc']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ACIM GEMM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

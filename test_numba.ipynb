{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def get_bit(input, bit_pos):\n",
    "  input = input >> bit_pos\n",
    "  return input & 1\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def get_bitplane(input, bit_pos):\n",
    "  mask = 1 << bit_pos\n",
    "  return (input & mask) >> bit_pos\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def adaptive_quantize(input):\n",
    "  max_val = 15\n",
    "\n",
    "  if input > max_val:\n",
    "    # div = torch.pow(torch.ceil(torch.log2(input/max_val)), 2)\n",
    "    div = 2**(math.ceil(math.log2(input/max_val)))\n",
    "  else:\n",
    "    div = 1\n",
    "\n",
    "  outval = np.int32(round(input/div) * div)\n",
    "  return outval\n",
    "\n",
    "def adaptive_quantize_(input):\n",
    "  max_val = 15\n",
    "\n",
    "  if input > max_val:\n",
    "    # div = torch.pow(torch.ceil(torch.log2(input/max_val)), 2)\n",
    "    div = 2**(math.ceil(math.log2(input/max_val)))\n",
    "  else:\n",
    "    div = 1\n",
    "\n",
    "  outval = np.int32(round(input/div) * div)\n",
    "  return outval\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def dot_acim(input, weight, in_bw, w_bw, quant=True):\n",
    "  assert len(input.shape) == 1\n",
    "  assert len(weight.shape) == 1\n",
    "  assert input.shape[0] == weight.shape[0]\n",
    "\n",
    "  out = 0\n",
    "  for in_bw_ in range(in_bw):\n",
    "    in_vec_bp = get_bitplane(input, in_bw_)\n",
    "    for w_bw_ in range(w_bw):\n",
    "      w_vec_bp = get_bitplane(weight, w_bw_)\n",
    "      out_value = np.dot(in_vec_bp, w_vec_bp, dtype=np.int32)\n",
    "      if quant:\n",
    "        dq_out_value = adaptive_quantize(out_value)\n",
    "      else:\n",
    "        dq_out_value = out_value\n",
    "\n",
    "      if in_bw_ == (in_bw-1): # negative input bit\n",
    "        dq_out_value = -dq_out_value\n",
    "      \n",
    "      if w_bw_ == (w_bw-1): # negative weight bit\n",
    "        dq_out_value = -dq_out_value\n",
    "      \n",
    "      out += (dq_out_value << (in_bw_ + w_bw_))\n",
    "\n",
    "  return out\n",
    "\n",
    "@cuda.jit\n",
    "def gemm_acim(input, weight, out, in_bw, w_bw, quant=True):\n",
    "  assert len(input.shape) == 2\n",
    "  assert len(weight.shape) == 2\n",
    "  assert input.shape[1] == weight.shape[0]\n",
    "\n",
    "  # out = np.zeros(input.shape[0], weight.shape[1], dtype=torch.int32)\n",
    "  row = cuda.blockDim.x * cuda.blockIdx.x + cuda.threadIdx.x\n",
    "  col = cuda.blockDim.y * cuda.blockIdx.y + cuda.threadIdx.y\n",
    "  if row < input.shape[0] and col < weight.shape[1]:\n",
    "    out[row, col] += dot_acim(input[row, :], weight[:, col], in_bw, w_bw, quant)\n",
    "  \n",
    "\n",
    "@cuda.jit\n",
    "def call_get_bit(input, output, bit_pos):\n",
    "  row = cuda.blockDim.x * cuda.blockIdx.x + cuda.threadIdx.x\n",
    "  if row < input.shape[0]:\n",
    "    output[row] = get_bit(input[row], bit_pos)\n",
    "\n",
    "@cuda.jit\n",
    "def call_get_bitplane(input, output, bit_pos):\n",
    "  row = cuda.blockDim.x * cuda.blockIdx.x + cuda.threadIdx.x\n",
    "  if row < input.shape[0]:\n",
    "    output[row] = get_bitplane(input[row], bit_pos)\n",
    "  # output[:] = get_bitplane(input, bit_pos)\n",
    "\n",
    "@cuda.jit\n",
    "def call_adap_quantize(input, output):\n",
    "  row = cuda.blockDim.x * cuda.blockIdx.x + cuda.threadIdx.x\n",
    "  if row < input.shape[0]:\n",
    "    output[row] = adaptive_quantize(input[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "y: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "out: [0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "N = 16\n",
    "x = np.arange(N, dtype=np.int32)\n",
    "y = np.arange(N, dtype=np.int32)\n",
    "out = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "# Allocate device memory and copy data to the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(out)\n",
    "\n",
    "# Define the number of threads per block and number of blocks per grid\n",
    "threads_per_block = 16 \n",
    "blocks_per_grid = 1\n",
    "\n",
    "# Launch the kernel\n",
    "call_get_bit[blocks_per_grid, threads_per_block](d_x, d_out, 1)\n",
    "\n",
    "# Copy the result back to the host\n",
    "out = d_out.copy_to_host()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"out:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "y: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "out: [0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "N = 16\n",
    "x = np.arange(N, dtype=np.int32)\n",
    "y = np.arange(N, dtype=np.int32)\n",
    "out = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "# Allocate device memory and copy data to the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(out)\n",
    "\n",
    "# Define the number of threads per block and number of blocks per grid\n",
    "threads_per_block = 16 \n",
    "blocks_per_grid = 1\n",
    "\n",
    "# Launch the kernel\n",
    "call_get_bitplane[blocks_per_grid, threads_per_block](d_x, d_out, 1)\n",
    "\n",
    "# Copy the result back to the host\n",
    "out = d_out.copy_to_host()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"out:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native function time: 0.000223 seconds\n",
      "Numba JIT function time: 0.005694 seconds\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "x = np.arange(N, dtype=np.int32)\n",
    "y = np.arange(N, dtype=np.int32)\n",
    "out = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "# Allocate device memory and copy data to the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(out)\n",
    "\n",
    "# Define the number of threads per block and number of blocks per grid\n",
    "threads_per_block = 128 \n",
    "blocks_per_grid = math.ceil(N/threads_per_block)\n",
    "\n",
    "# Copy the result back to the host\n",
    "out = d_out.copy_to_host()\n",
    "\n",
    "# print(\"x:\", x)\n",
    "# print(\"y:\", y)\n",
    "# print(\"out:\", out)\n",
    "\n",
    "# Measure time for native function\n",
    "start_time = time.time()\n",
    "call_adap_quantize[blocks_per_grid, threads_per_block](d_x, d_out)\n",
    "native_time = time.time() - start_time\n",
    "print(f\"Native function time: {native_time:.6f} seconds\")\n",
    "\n",
    "# Measure time for Numba JIT function\n",
    "start_time = time.time()\n",
    "for i in range(N):\n",
    "  adaptive_quantize_(x[i])\n",
    "numba_time = time.time() - start_time\n",
    "print(f\"Numba JIT function time: {numba_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mFailed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mFailed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mNo implementation of function Function(<built-in function and_>) found for signature:\n \n >>> and_(array(int32, 1d, C), int64)\n \nThere are 4 candidate implementations:\n\u001b[1m      - Of which 2 did not match due to:\n      Operator Overload in function 'and_': File: unknown: Line unknown.\n        With argument(s): '(array(int32, 1d, C), int64)':\u001b[0m\n\u001b[1m       No match for registered cases:\n        * (bool, bool) -> bool\n        * (int64, int64) -> int64\n        * (int64, uint64) -> int64\n        * (uint64, int64) -> int64\n        * (uint64, uint64) -> uint64\u001b[0m\n\u001b[1m      - Of which 2 did not match due to:\n      Overload of function 'and_': File: numba/experimental/jitclass/overloads.py: Line 0.\n        With argument(s): '(array(int32, 1d, C), int64)':\u001b[0m\n\u001b[1m       No match.\u001b[0m\n\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of intrinsic-call at /tmp/ipykernel_2665584/2902190974.py (9)\u001b[0m\n\u001b[1m\nFile \"../../../tmp/ipykernel_2665584/2902190974.py\", line 9:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\n\u001b[0m\u001b[1mDuring: resolving callee type: type(CUDADispatcher(<function get_bitplane at 0x7f22a5fd4310>))\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_2665584/2902190974.py (32)\n\u001b[0m\n\u001b[1m\nFile \"../../../tmp/ipykernel_2665584/2902190974.py\", line 32:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\n\u001b[0m\u001b[1mDuring: resolving callee type: type(CUDADispatcher(<function dot_acim at 0x7f22a5fd6b00>))\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_2665584/2902190974.py (61)\n\u001b[0m\n\u001b[1m\nFile \"../../../tmp/ipykernel_2665584/2902190974.py\", line 61:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m blocks_per_grid \u001b[38;5;241m=\u001b[39m (math\u001b[38;5;241m.\u001b[39mceil(M\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m), math\u001b[38;5;241m.\u001b[39mceil(N\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Launch the kernel\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mgemm_acim\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks_per_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads_per_block\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Copy the result back to the host\u001b[39;00m\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m d_out\u001b[38;5;241m.\u001b[39mcopy_to_host()\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:539\u001b[0m, in \u001b[0;36m_LaunchConfiguration.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:681\u001b[0m, in \u001b[0;36mCUDADispatcher.call\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    679\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverloads\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m kernel\u001b[38;5;241m.\u001b[39mlaunch(args, griddim, blockdim, stream, sharedmem)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:689\u001b[0m, in \u001b[0;36mCUDADispatcher._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kws\n\u001b[1;32m    688\u001b[0m argtypes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypeof_pyval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:932\u001b[0m, in \u001b[0;36mCUDADispatcher.compile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_compile:\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilation disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 932\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_Kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetoptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[1;32m    934\u001b[0m kernel\u001b[38;5;241m.\u001b[39mbind()\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:83\u001b[0m, in \u001b[0;36m_Kernel.__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m nvvm_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastmath\u001b[39m\u001b[38;5;124m'\u001b[39m: fastmath,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     80\u001b[0m }\n\u001b[1;32m     82\u001b[0m cc \u001b[38;5;241m=\u001b[39m get_current_device()\u001b[38;5;241m.\u001b[39mcompute_capability\n\u001b[0;32m---> 83\u001b[0m cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlineinfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineinfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfastmath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfastmath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnvvm_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnvvm_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m tgt_ctx \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mtarget_context\n\u001b[1;32m     91\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/compiler.py:196\u001b[0m, in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options, cc)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtarget_extension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m target_override\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m target_override(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m     cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypingctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypingctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtargetctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargetctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCUDACompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m library \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mlibrary\n\u001b[1;32m    206\u001b[0m library\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler.py:751\u001b[0m, in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compiler entry point\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03mParameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    compiler pipeline\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    749\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    750\u001b[0m                           args, return_type, flags, \u001b[38;5;28mlocals\u001b[39m)\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler.py:445\u001b[0m, in \u001b[0;36mCompilerBase.compile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted_from \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler.py:513\u001b[0m, in \u001b[0;36mCompilerBase._compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03mPopulate and run pipeline for bytecode input\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfunc_ir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler.py:492\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mfail_reason \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_final_pipeline:\n\u001b[0;32m--> 492\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompilerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll available pipelines exhausted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler.py:479\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_machinery.py:368\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    365\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m mode pipeline (step: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \\\n\u001b[1;32m    366\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, pass_desc)\n\u001b[1;32m    367\u001b[0m patched_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_error(msg, e)\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m patched_exception\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_machinery.py:356\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    354\u001b[0m pass_inst \u001b[38;5;241m=\u001b[39m _pass_registry\u001b[38;5;241m.\u001b[39mget(pss)\u001b[38;5;241m.\u001b[39mpass_inst\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pass_inst, CompilerPass):\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runPass\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegacy pass in use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_machinery.py:311\u001b[0m, in \u001b[0;36mPassManager._runPass\u001b[0;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_initialization, internal_state)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m pass_time:\n\u001b[0;32m--> 311\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m finalize_time:\n\u001b[1;32m    313\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_finalizer, internal_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/compiler_machinery.py:273\u001b[0m, in \u001b[0;36mPassManager._runPass.<locals>.check\u001b[0;34m(func, compiler_state)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(func, compiler_state):\n\u001b[0;32m--> 273\u001b[0m     mangled \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mangled \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    275\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass implementations should return True/False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m did not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/typed_passes.py:112\u001b[0m, in \u001b[0;36mBaseTypeInference.run_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mType inference and legalization\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fallback_context(state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m failed type inference\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    110\u001b[0m                       \u001b[38;5;241m%\u001b[39m (state\u001b[38;5;241m.\u001b[39mfunc_id\u001b[38;5;241m.\u001b[39mfunc_name,)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# Type inference\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     typemap, return_type, calltypes, errs \u001b[38;5;241m=\u001b[39m \u001b[43mtype_inference_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypingctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_ir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     state\u001b[38;5;241m.\u001b[39mtypemap \u001b[38;5;241m=\u001b[39m typemap\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# save errors in case of partial typing\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/typed_passes.py:93\u001b[0m, in \u001b[0;36mtype_inference_stage\u001b[0;34m(typingctx, targetctx, interp, args, return_type, locals, raise_errors)\u001b[0m\n\u001b[1;32m     91\u001b[0m     infer\u001b[38;5;241m.\u001b[39mbuild_constraint()\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# return errors in case of partial typing\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     errs \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     typemap, restype, calltypes \u001b[38;5;241m=\u001b[39m infer\u001b[38;5;241m.\u001b[39munify(raise_errors\u001b[38;5;241m=\u001b[39mraise_errors)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _TypingResults(typemap, restype, calltypes, errs)\n",
      "File \u001b[0;32m~/anaconda3/envs/atom/lib/python3.10/site-packages/numba/core/typeinfer.py:1091\u001b[0m, in \u001b[0;36mTypeInferer.propagate\u001b[0;34m(self, raise_errors)\u001b[0m\n\u001b[1;32m   1088\u001b[0m force_lit_args \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m errors\n\u001b[1;32m   1089\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ForceLiteralArg)]\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_lit_args:\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m reduce(operator\u001b[38;5;241m.\u001b[39mor_, force_lit_args)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mFailed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mFailed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mNo implementation of function Function(<built-in function and_>) found for signature:\n \n >>> and_(array(int32, 1d, C), int64)\n \nThere are 4 candidate implementations:\n\u001b[1m      - Of which 2 did not match due to:\n      Operator Overload in function 'and_': File: unknown: Line unknown.\n        With argument(s): '(array(int32, 1d, C), int64)':\u001b[0m\n\u001b[1m       No match for registered cases:\n        * (bool, bool) -> bool\n        * (int64, int64) -> int64\n        * (int64, uint64) -> int64\n        * (uint64, int64) -> int64\n        * (uint64, uint64) -> uint64\u001b[0m\n\u001b[1m      - Of which 2 did not match due to:\n      Overload of function 'and_': File: numba/experimental/jitclass/overloads.py: Line 0.\n        With argument(s): '(array(int32, 1d, C), int64)':\u001b[0m\n\u001b[1m       No match.\u001b[0m\n\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of intrinsic-call at /tmp/ipykernel_2665584/2902190974.py (9)\u001b[0m\n\u001b[1m\nFile \"../../../tmp/ipykernel_2665584/2902190974.py\", line 9:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\n\u001b[0m\u001b[1mDuring: resolving callee type: type(CUDADispatcher(<function get_bitplane at 0x7f22a5fd4310>))\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_2665584/2902190974.py (32)\n\u001b[0m\n\u001b[1m\nFile \"../../../tmp/ipykernel_2665584/2902190974.py\", line 32:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\n\u001b[0m\u001b[1mDuring: resolving callee type: type(CUDADispatcher(<function dot_acim at 0x7f22a5fd6b00>))\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_2665584/2902190974.py (61)\n\u001b[0m\n\u001b[1m\nFile \"../../../tmp/ipykernel_2665584/2902190974.py\", line 61:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "M = 16\n",
    "N = 16\n",
    "K = 16\n",
    "x = np.arange(M*K, dtype=np.int32).reshape(M, K)\n",
    "y = np.arange(K*N, dtype=np.int32).reshape(K, N)\n",
    "out = np.zeros(M*N, dtype=np.int32).reshape(M, N)\n",
    "\n",
    "# Allocate device memory and copy data to the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(out)\n",
    "\n",
    "# Define the number of threads per block and number of blocks per grid\n",
    "threads_per_block = (32, 32) \n",
    "blocks_per_grid = (math.ceil(M/32), math.ceil(N/32))\n",
    "\n",
    "# Launch the kernel\n",
    "gemm_acim[blocks_per_grid, threads_per_block](d_x, d_y, d_out, 4, 4, True)\n",
    "\n",
    "# Copy the result back to the host\n",
    "out = d_out.copy_to_host()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"out:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "y: [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "out: [[ 56  62  68  74]\n",
      " [152 174 196 218]\n",
      " [248 286 324 362]\n",
      " [344 398 452 506]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/atom/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit(device=True)\n",
    "def mul(x, y):\n",
    "    return x * y\n",
    "\n",
    "@cuda.jit\n",
    "def mm_kernel(x, y, out):\n",
    "    idx = cuda.grid(1)\n",
    "    row = cuda.blockDim.x * cuda.blockIdx.x + cuda.threadIdx.x\n",
    "    col = cuda.blockDim.y * cuda.blockIdx.y + cuda.threadIdx.y\n",
    "    if row < x.shape[0] and col < y.shape[1]:\n",
    "        tmp = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            # tmp += x[row, i] * y[i, col]\n",
    "            tmp += mul(x[row, i], y[i, col])\n",
    "        out[row, col] = tmp\n",
    "\n",
    "# Initialize data\n",
    "N = 16\n",
    "x = np.arange(N, dtype=np.int32).reshape(4,4)\n",
    "y = np.arange(N, dtype=np.int32).reshape(4,4)\n",
    "out = np.zeros(N, dtype=np.int32).reshape(4,4)\n",
    "\n",
    "# Allocate device memory and copy data to the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(out)\n",
    "\n",
    "# Define the number of threads per block and number of blocks per grid\n",
    "threads_per_block = (4,4)\n",
    "blocks_per_grid = (1,1)\n",
    "\n",
    "# Launch the kernel\n",
    "mm_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n",
    "\n",
    "# Copy the result back to the host\n",
    "out = d_out.copy_to_host()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"out:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56,  62,  68,  74],\n",
       "       [152, 174, 196, 218],\n",
       "       [248, 286, 324, 362],\n",
       "       [344, 398, 452, 506]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
